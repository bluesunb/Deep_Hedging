{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from Algorithms.ddpg import config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Plot Setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib\n",
    "\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "FONTSIZE = 10\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "matplotlib.rcParams[\"figure.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"legend.fontsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "matplotlib.rcParams[\"savefig.pad_inches\"] = 0.1\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2\n",
    "matplotlib.rcParams[\"axes.linewidth\"] = 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Model Setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Load Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env 'BSMarket was created!\n",
      "env 'BSMarket was created!\n"
     ]
    }
   ],
   "source": [
    "env_kwargs, model_kwargs, learn_kwargs = config.load_config('tmp_config.yaml')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "ntb_mode = True\n",
    "\n",
    "model_kwargs.update({\n",
    "    'buffer_size': 300,\n",
    "    'learning_starts': 300,\n",
    "    'batch_size': 15,\n",
    "    'std_coeff': env_kwargs['cost']\n",
    "})\n",
    "\n",
    "model_kwargs['policy_kwargs'].update({\n",
    "    'ntb_mode': ntb_mode\n",
    "})\n",
    "\n",
    "learn_kwargs.update({\n",
    "    'total_timesteps': 1500\n",
    "})\n",
    "\n",
    "if ntb_mode:\n",
    "    actor_net_kwargs = {'bn_kwargs': {'num_features': env_kwargs['n_assets']}}\n",
    "    critic_net_kwargs = {'bn_kwargs': {'num_features': env_kwargs['n_assets']}}\n",
    "\n",
    "    model_kwargs['policy_kwargs'].update({\n",
    "        'net_arch': {'pi': [(nn.BatchNorm1d, 'bn'), 32, 32, 2],\n",
    "                     'qf': [(nn.BatchNorm1d, 'bn'), 2]},\n",
    "        'actor_net_kwargs': actor_net_kwargs,\n",
    "        'critic_net_kwargs': critic_net_kwargs,\n",
    "    })\n",
    "\n",
    "    model_kwargs['policy_kwargs']['features_extractor_kwargs'].update({\n",
    "        'features_out': 64,\n",
    "        'net_arch': [32]\n",
    "    })\n",
    "\n",
    "else:\n",
    "    model_kwargs['policy_kwargs'].update({\n",
    "        'net_arch': [],\n",
    "    })\n",
    "\n",
    "    model_kwargs['policy_kwargs']['features_extractor_kwargs'].update({\n",
    "        'features_out': 2,\n",
    "        'net_arch': [32, 64]\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env 'BSMarket was created!\n",
      "model_kwargs['env']: <BSMarket instance>\n",
      "env 'BSMarket was created!\n",
      "learn_kwargs['eval_env']: <BSMarketEval instance>\n",
      "learn_kwargs['tb_log_name']: ddpg_220608-0002\n",
      "learn_kwargs['eval_log_path']: ../logs/tb_logs/ddpg_220608-0002_1\n"
     ]
    }
   ],
   "source": [
    "config.reconstruct_config(env_kwargs, model_kwargs, learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost': 0.02,\n",
      " 'dividend': 0.0,\n",
      " 'drift': 0.0,\n",
      " 'freq': 1,\n",
      " 'gen_name': 'gbm',\n",
      " 'init_price': 1.0,\n",
      " 'n_assets': 1000,\n",
      " 'n_periods': 30,\n",
      " 'payoff': 'european',\n",
      " 'period_unit': 365,\n",
      " 'reward_mode': 'pnl',\n",
      " 'risk_free_interest': 0.0,\n",
      " 'strike': 1.0,\n",
      " 'volatility': 0.2}\n"
     ]
    }
   ],
   "source": [
    "pprint(env_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_noise': NormalActionNoise(mu=0.0, sigma=0.1),\n",
      " 'batch_size': 15,\n",
      " 'buffer_size': 300,\n",
      " 'create_eval_env': False,\n",
      " 'device': 'auto',\n",
      " 'env': <Env.env.BSMarket object at 0x0000020096B48E80>,\n",
      " 'gamma': 0.99,\n",
      " 'gradient_steps': -1,\n",
      " 'learning_rate': <function lr_schedule at 0x000001FF9DC8AEE0>,\n",
      " 'learning_starts': 300,\n",
      " 'optimize_memory_usage': False,\n",
      " 'policy': <class 'Algorithms.ddpg.policies_legacy.DoubleDDPGPolicy'>,\n",
      " 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                   'actor_net_kwargs': {'bn_kwargs': {'num_features': 1000}},\n",
      "                   'critic_net_kwargs': {'bn_kwargs': {'num_features': 1000}},\n",
      "                   'features_extractor_class': <class 'Env.feature_extractor.MarketObsExtractor'>,\n",
      "                   'features_extractor_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                                                 'features_in': 4,\n",
      "                                                 'features_out': 64,\n",
      "                                                 'last_activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                                                 'net_arch': [32]},\n",
      "                   'n_critics': 1,\n",
      "                   'net_arch': {'pi': [(<class 'torch.nn.modules.batchnorm.BatchNorm1d'>,\n",
      "                                        'bn'),\n",
      "                                       32,\n",
      "                                       32,\n",
      "                                       2],\n",
      "                                'qf': [(<class 'torch.nn.modules.batchnorm.BatchNorm1d'>,\n",
      "                                        'bn'),\n",
      "                                       2]},\n",
      "                   'normalize_images': False,\n",
      "                   'ntb_mode': True,\n",
      "                   'optimizer_class': <class 'torch.optim.adam.Adam'>,\n",
      "                   'optimizer_kwargs': None,\n",
      "                   'share_features_extractor': True},\n",
      " 'replay_buffer_class': <class 'Env.buffers.CustomReplayBuffer'>,\n",
      " 'replay_buffer_kwargs': {},\n",
      " 'seed': 42,\n",
      " 'std_coeff': 0.02,\n",
      " 'tau': 0.005,\n",
      " 'tensorboard_log': '../logs/tb_logs',\n",
      " 'train_freq': (1, 'episode'),\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "pprint(model_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'callback': <Algorithms.ddpg.callbacks.ReportCallbacks object at 0x0000020096B558E0>,\n",
      " 'eval_env': <Env.env.BSMarketEval object at 0x0000020096B48AC0>,\n",
      " 'eval_freq': 30,\n",
      " 'eval_log_path': '../logs/tb_logs/ddpg_220608-0002_1',\n",
      " 'log_interval': 30,\n",
      " 'n_eval_episodes': 1,\n",
      " 'reset_num_timesteps': True,\n",
      " 'tb_log_name': 'ddpg_220608-0002',\n",
      " 'total_timesteps': 1500}\n"
     ]
    }
   ],
   "source": [
    "pprint(learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Make env, model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from Algorithms.ddpg import DDPG\n",
    "# from Algorithms.ddpg.double_ddpg import DDPG\n",
    "\n",
    "model = DDPG(**model_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "DoubleDDPGPolicy(\n  (actor): CustomActor(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n      )\n    )\n    (mu): Sequential(\n      (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=64, out_features=32, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=32, out_features=32, bias=True)\n      (4): ReLU()\n      (5): Linear(in_features=32, out_features=2, bias=True)\n      (6): ReLU()\n      (7): Linear(in_features=2, out_features=2, bias=True)\n      (8): Tanh()\n    )\n    (flatten): Flatten(start_dim=-2, end_dim=-1)\n  )\n  (actor_target): CustomActor(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n      )\n    )\n    (mu): Sequential(\n      (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=64, out_features=32, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=32, out_features=32, bias=True)\n      (4): ReLU()\n      (5): Linear(in_features=32, out_features=2, bias=True)\n      (6): ReLU()\n      (7): Linear(in_features=2, out_features=2, bias=True)\n      (8): Tanh()\n    )\n    (flatten): Flatten(start_dim=-2, end_dim=-1)\n  )\n  (critic): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=65, out_features=2, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=2, out_features=1, bias=True)\n    )\n  )\n  (critic2): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=65, out_features=2, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=2, out_features=1, bias=True)\n    )\n  )\n  (critic_target): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=65, out_features=2, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=2, out_features=1, bias=True)\n    )\n  )\n  (critic2_target): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=65, out_features=2, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=2, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ../logs/tb_logs\\ddpg_220607-2339_1\n",
      "[Training Start]\n",
      "Eval num_timesteps=30, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0173  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 30       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0187  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0196  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.02    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 120      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0188  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 150      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.021   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0187  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 210      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0171  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 240      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0182  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 270      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0174  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 300      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0209  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 330      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00425 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 360      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.199    |\n",
      "|    critic2_loss    | 0.113    |\n",
      "|    critic_loss     | 0.0937   |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    mean_cost_loss  | -0.187   |\n",
      "|    n_updates       | 30       |\n",
      "|    std_cost_loss   | 0.0123   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00328 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 390      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.199    |\n",
      "|    critic2_loss    | 0.106    |\n",
      "|    critic_loss     | 0.0981   |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    mean_cost_loss  | -0.188   |\n",
      "|    n_updates       | 60       |\n",
      "|    std_cost_loss   | 0.011    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=420, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00358 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 420      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.199    |\n",
      "|    critic2_loss    | 0.104    |\n",
      "|    critic_loss     | 0.0956   |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    mean_cost_loss  | -0.188   |\n",
      "|    n_updates       | 90       |\n",
      "|    std_cost_loss   | 0.0113   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00305 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 450      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.209    |\n",
      "|    critic2_loss    | 0.101    |\n",
      "|    critic_loss     | 0.0916   |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    mean_cost_loss  | -0.197   |\n",
      "|    n_updates       | 120      |\n",
      "|    std_cost_loss   | 0.0115   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00198 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 480      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.212    |\n",
      "|    critic2_loss    | 0.0995   |\n",
      "|    critic_loss     | 0.0909   |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    mean_cost_loss  | -0.2     |\n",
      "|    n_updates       | 150      |\n",
      "|    std_cost_loss   | 0.0116   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=510, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00311 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 510      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.216    |\n",
      "|    critic2_loss    | 0.0986   |\n",
      "|    critic_loss     | 0.0896   |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    mean_cost_loss  | -0.204   |\n",
      "|    n_updates       | 180      |\n",
      "|    std_cost_loss   | 0.0116   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00285 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 540      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.217    |\n",
      "|    critic2_loss    | 0.0968   |\n",
      "|    critic_loss     | 0.0897   |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    mean_cost_loss  | -0.205   |\n",
      "|    n_updates       | 210      |\n",
      "|    std_cost_loss   | 0.0117   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00257 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 570      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.214    |\n",
      "|    critic2_loss    | 0.0974   |\n",
      "|    critic_loss     | 0.0873   |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    mean_cost_loss  | -0.202   |\n",
      "|    n_updates       | 240      |\n",
      "|    std_cost_loss   | 0.0117   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00359 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.216    |\n",
      "|    critic2_loss    | 0.0966   |\n",
      "|    critic_loss     | 0.0872   |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    mean_cost_loss  | -0.204   |\n",
      "|    n_updates       | 270      |\n",
      "|    std_cost_loss   | 0.0117   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00153 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 630      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.214    |\n",
      "|    critic2_loss    | 0.0963   |\n",
      "|    critic_loss     | 0.0876   |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    mean_cost_loss  | -0.202   |\n",
      "|    n_updates       | 300      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=660, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00321 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 660      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.213    |\n",
      "|    critic2_loss    | 0.0953   |\n",
      "|    critic_loss     | 0.0869   |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    mean_cost_loss  | -0.201   |\n",
      "|    n_updates       | 330      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00175 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 690      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.214    |\n",
      "|    critic2_loss    | 0.0939   |\n",
      "|    critic_loss     | 0.087    |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    mean_cost_loss  | -0.202   |\n",
      "|    n_updates       | 360      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00222 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 720      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.211    |\n",
      "|    critic2_loss    | 0.0944   |\n",
      "|    critic_loss     | 0.0832   |\n",
      "|    learning_rate   | 0.000979 |\n",
      "|    mean_cost_loss  | -0.199   |\n",
      "|    n_updates       | 390      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00313 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 750      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.215    |\n",
      "|    critic2_loss    | 0.0927   |\n",
      "|    critic_loss     | 0.0865   |\n",
      "|    learning_rate   | 0.000932 |\n",
      "|    mean_cost_loss  | -0.203   |\n",
      "|    n_updates       | 420      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0041  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 780      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.212    |\n",
      "|    critic2_loss    | 0.0932   |\n",
      "|    critic_loss     | 0.0845   |\n",
      "|    learning_rate   | 0.000889 |\n",
      "|    mean_cost_loss  | -0.201   |\n",
      "|    n_updates       | 450      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 30        |\n",
      "|    mean_reward     | -0.000331 |\n",
      "| time/              |           |\n",
      "|    total timesteps | 810       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 0.215     |\n",
      "|    critic2_loss    | 0.0912    |\n",
      "|    critic_loss     | 0.0862    |\n",
      "|    learning_rate   | 0.00085   |\n",
      "|    mean_cost_loss  | -0.203    |\n",
      "|    n_updates       | 480       |\n",
      "|    std_cost_loss   | 0.0118    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=840, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00238 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 840      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.211    |\n",
      "|    critic2_loss    | 0.0931   |\n",
      "|    critic_loss     | 0.0828   |\n",
      "|    learning_rate   | 0.000814 |\n",
      "|    mean_cost_loss  | -0.199   |\n",
      "|    n_updates       | 510      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00231 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 870      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic2_loss    | 0.0927   |\n",
      "|    critic_loss     | 0.0811   |\n",
      "|    learning_rate   | 0.000781 |\n",
      "|    mean_cost_loss  | -0.197   |\n",
      "|    n_updates       | 540      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00399 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 900      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.214    |\n",
      "|    critic2_loss    | 0.092    |\n",
      "|    critic_loss     | 0.0846   |\n",
      "|    learning_rate   | 0.000751 |\n",
      "|    mean_cost_loss  | -0.202   |\n",
      "|    n_updates       | 570      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.0843  |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 16       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total timesteps | 900      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00311 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 930      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.209    |\n",
      "|    critic2_loss    | 0.0929   |\n",
      "|    critic_loss     | 0.0818   |\n",
      "|    learning_rate   | 0.000723 |\n",
      "|    mean_cost_loss  | -0.197   |\n",
      "|    n_updates       | 600      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 30        |\n",
      "|    mean_reward     | -0.000574 |\n",
      "| time/              |           |\n",
      "|    total timesteps | 960       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 0.21      |\n",
      "|    critic2_loss    | 0.092     |\n",
      "|    critic_loss     | 0.0823    |\n",
      "|    learning_rate   | 0.000697  |\n",
      "|    mean_cost_loss  | -0.198    |\n",
      "|    n_updates       | 630       |\n",
      "|    std_cost_loss   | 0.0118    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=990, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00187 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 990      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.211    |\n",
      "|    critic2_loss    | 0.0919   |\n",
      "|    critic_loss     | 0.0833   |\n",
      "|    learning_rate   | 0.000674 |\n",
      "|    mean_cost_loss  | -0.199   |\n",
      "|    n_updates       | 660      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0039  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.213    |\n",
      "|    critic2_loss    | 0.0909   |\n",
      "|    critic_loss     | 0.0851   |\n",
      "|    learning_rate   | 0.000652 |\n",
      "|    mean_cost_loss  | -0.201   |\n",
      "|    n_updates       | 690      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00328 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.21     |\n",
      "|    critic2_loss    | 0.0923   |\n",
      "|    critic_loss     | 0.0834   |\n",
      "|    learning_rate   | 0.000633 |\n",
      "|    mean_cost_loss  | -0.199   |\n",
      "|    n_updates       | 720      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00101 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic2_loss    | 0.0931   |\n",
      "|    critic_loss     | 0.0818   |\n",
      "|    learning_rate   | 0.000615 |\n",
      "|    mean_cost_loss  | -0.196   |\n",
      "|    n_updates       | 750      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00242 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.21     |\n",
      "|    critic2_loss    | 0.0922   |\n",
      "|    critic_loss     | 0.0839   |\n",
      "|    learning_rate   | 0.000599 |\n",
      "|    mean_cost_loss  | -0.199   |\n",
      "|    n_updates       | 780      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00141 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.212    |\n",
      "|    critic2_loss    | 0.0909   |\n",
      "|    critic_loss     | 0.0852   |\n",
      "|    learning_rate   | 0.000584 |\n",
      "|    mean_cost_loss  | -0.2     |\n",
      "|    n_updates       | 810      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00147 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic2_loss    | 0.0932   |\n",
      "|    critic_loss     | 0.0825   |\n",
      "|    learning_rate   | 0.000571 |\n",
      "|    mean_cost_loss  | -0.196   |\n",
      "|    n_updates       | 840      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00238 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic2_loss    | 0.093    |\n",
      "|    critic_loss     | 0.0834   |\n",
      "|    learning_rate   | 0.000559 |\n",
      "|    mean_cost_loss  | -0.196   |\n",
      "|    n_updates       | 870      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0032  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.211    |\n",
      "|    critic2_loss    | 0.0915   |\n",
      "|    critic_loss     | 0.0856   |\n",
      "|    learning_rate   | 0.000548 |\n",
      "|    mean_cost_loss  | -0.199   |\n",
      "|    n_updates       | 900      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00327 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic2_loss    | 0.0929   |\n",
      "|    critic_loss     | 0.0837   |\n",
      "|    learning_rate   | 0.000539 |\n",
      "|    mean_cost_loss  | -0.196   |\n",
      "|    n_updates       | 930      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00235 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.212    |\n",
      "|    critic2_loss    | 0.0913   |\n",
      "|    critic_loss     | 0.0874   |\n",
      "|    learning_rate   | 0.00053  |\n",
      "|    mean_cost_loss  | -0.2     |\n",
      "|    n_updates       | 960      |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.002   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.207    |\n",
      "|    critic2_loss    | 0.0933   |\n",
      "|    critic_loss     | 0.0834   |\n",
      "|    learning_rate   | 0.000523 |\n",
      "|    mean_cost_loss  | -0.195   |\n",
      "|    n_updates       | 990      |\n",
      "|    std_cost_loss   | 0.0117   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00277 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.206    |\n",
      "|    critic2_loss    | 0.0934   |\n",
      "|    critic_loss     | 0.0831   |\n",
      "|    learning_rate   | 0.000517 |\n",
      "|    mean_cost_loss  | -0.194   |\n",
      "|    n_updates       | 1020     |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00296 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic2_loss    | 0.0936   |\n",
      "|    critic_loss     | 0.0858   |\n",
      "|    learning_rate   | 0.000512 |\n",
      "|    mean_cost_loss  | -0.197   |\n",
      "|    n_updates       | 1050     |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00081 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.207    |\n",
      "|    critic2_loss    | 0.0931   |\n",
      "|    critic_loss     | 0.0855   |\n",
      "|    learning_rate   | 0.000507 |\n",
      "|    mean_cost_loss  | -0.195   |\n",
      "|    n_updates       | 1080     |\n",
      "|    std_cost_loss   | 0.0117   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00278 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic2_loss    | 0.0922   |\n",
      "|    critic_loss     | 0.0856   |\n",
      "|    learning_rate   | 0.000504 |\n",
      "|    mean_cost_loss  | -0.196   |\n",
      "|    n_updates       | 1110     |\n",
      "|    std_cost_loss   | 0.0118   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00305 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.205    |\n",
      "|    critic2_loss    | 0.0947   |\n",
      "|    critic_loss     | 0.0843   |\n",
      "|    learning_rate   | 0.000502 |\n",
      "|    mean_cost_loss  | -0.194   |\n",
      "|    n_updates       | 1140     |\n",
      "|    std_cost_loss   | 0.0117   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00232 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.204    |\n",
      "|    critic2_loss    | 0.0948   |\n",
      "|    critic_loss     | 0.0835   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    mean_cost_loss  | -0.192   |\n",
      "|    n_updates       | 1170     |\n",
      "|    std_cost_loss   | 0.0117   |\n",
      "---------------------------------\n",
      "[Training End]  steps: 1500\ttimes: 92.92043471336365\n"
     ]
    }
   ],
   "source": [
    "model = model.learn(**learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BSMarket instance> will be save as name. env_kwargs not in kwargs!\n",
      "<BSMarketEval instance> will be save as name. eval_env_kwargs not in kwargs!\n",
      "<Algorithms.ddpg.callbacks.ReportCallbacks object at 0x000001FF9DEA25B0> will be save as name. callback_kwargs not in kwargs!\n",
      "../logs/tb_logs/ddpg_220607-2339_1/config.yaml was saved.\n"
     ]
    }
   ],
   "source": [
    "config.save_config(f'{learn_kwargs[\"eval_log_path\"]}/config.yaml', env_kwargs, model_kwargs, learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. P&L Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# model = model.load('../logs/tb_logs/ddpg_220607-2214_stable'+'/best_model')\n",
    "model = model.load('../logs/tb_logs/ddpg_220607-2339_1'+'/best_model')\n",
    "# model = model.load(learn_kwargs['eval_log_path'] + '/best_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<Env.env.BSMarketEval at 0x20096b48ac0>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env = learn_kwargs['eval_env']\n",
    "eval_env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_pnl = np.mean([eval_env.pnl_eval() for _ in range(30)], axis=0)\n",
    "delta_pnl = np.mean([eval_env.delta_eval() for _ in range(30)], axis=0)\n",
    "rl_pnl = np.mean([eval_env.pnl_eval(model) for _ in range(30)], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "np.save('best_results/random_pnl', random_pnl)\n",
    "np.save('best_results/delta_pnl', delta_pnl)\n",
    "np.save('best_results/rl_pnl', rl_pnl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "ntb_pnl = np.mean([eval_env.pnl_eval(model) for _ in range(30)], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pnl_reward(pnl):\n",
    "    mean = np.mean(pnl)\n",
    "    std = np.std(pnl)\n",
    "    return mean - 0.02 * std , (mean, std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta:\t-0.0309, -0.0307, 0.0114\n",
      "ntb_pnl:\t\t0.0006, 0.0007, 0.0062\n",
      "rl:\t\t0.0007, 0.0008, 0.0064\n",
      "0.03144408392813277\n",
      "0.0001298123786257927\n"
     ]
    }
   ],
   "source": [
    "plt.xlabel('P&L')\n",
    "# plt.hist(random_pnl, bins=100, range=(-0.25, 0.05), alpha=0.6, label='random')\n",
    "# plt.hist(random_pnl, bins=100, range=(-0.2, 0.05), alpha=0.6, label='random')\n",
    "plt.hist(delta_pnl, bins=100, range=(-0.2, 0.05), alpha=0.6, label='delta')\n",
    "plt.hist(ntb_pnl, bins=100, range=(-0.2, 0.05), alpha=0.6, label='ntb')\n",
    "plt.hist(rl_pnl, bins=100, range=(-0.2, 0.05), alpha=0.6, label='rl')\n",
    "plt.ylim(0, 150)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "r1, (m1, s1) = pnl_reward(delta_pnl)\n",
    "r2, (m2, s2) = pnl_reward(ntb_pnl)\n",
    "r3, (m3, s3) = pnl_reward(rl_pnl)\n",
    "print(f'delta:\\t{r1:.4f}, {m1:.4f}, {s1:.4f}')\n",
    "print(f'ntb_pnl:\\t\\t{r2:.4f}, {m2:.4f}, {s2:.4f}')\n",
    "print(f'rl:\\t\\t{r3:.4f}, {m3:.4f}, {s3:.4f}')\n",
    "print((r2-r1))\n",
    "print((r3-r2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}