{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from Algorithms.ddpg import config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Plot Setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib\n",
    "\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "FONTSIZE = 10\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "matplotlib.rcParams[\"figure.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"legend.fontsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "matplotlib.rcParams[\"savefig.pad_inches\"] = 0.1\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2\n",
    "matplotlib.rcParams[\"axes.linewidth\"] = 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Model Setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Load Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env 'BSMarket was created!\n",
      "env 'BSMarket was created!\n"
     ]
    }
   ],
   "source": [
    "env_kwargs, model_kwargs, learn_kwargs = config.load_config('tmp_config.yaml')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from Algorithms.ddpg.policies import DoubleTD3Policy\n",
    "from Algorithms.ddpg.policies_legacy import DoubleDDPGPolicy\n",
    "\n",
    "model_kwargs.update({\n",
    "    'buffer_size': 300,\n",
    "    'learning_starts': 300,\n",
    "    'batch_size': 15,\n",
    "    'std_coeff': env_kwargs['cost']\n",
    "})\n",
    "\n",
    "model_kwargs['policy_kwargs'].update({\n",
    "    'net_arch': [],\n",
    "    'ntb_mode': False,\n",
    "})\n",
    "\n",
    "model_kwargs['policy_kwargs']['features_extractor_kwargs'].update({\n",
    "    'features_out': 2\n",
    "})\n",
    "\n",
    "learn_kwargs.update({\n",
    "    'total_timesteps': 1500\n",
    "})\n",
    "\n",
    "# model_kwargs.update({\n",
    "#     'policy': DoubleTD3Policy,\n",
    "#     'buffer_size': 300,\n",
    "#     'learning_starts': 300,\n",
    "#     'batch_size': 15,\n",
    "#     'std_coeff': env_kwargs['cost']\n",
    "# })\n",
    "#\n",
    "# del model_kwargs['policy_kwargs']['ntb_mode']\n",
    "# del model_kwargs['policy_kwargs']['actor_net_kwargs']\n",
    "# del model_kwargs['policy_kwargs']['critic_net_kwargs']\n",
    "#\n",
    "# model_kwargs['policy_kwargs'].update({\n",
    "#     'net_arch': []\n",
    "# })\n",
    "#\n",
    "# model_kwargs['policy_kwargs']['features_extractor_kwargs'].update({\n",
    "#     'features_out': 2,\n",
    "# })\n",
    "#\n",
    "# learn_kwargs.update({\n",
    "#     'total_timesteps': 1500\n",
    "# })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env 'BSMarket was created!\n",
      "model_kwargs['env']: <BSMarket instance>\n",
      "env 'BSMarket was created!\n",
      "learn_kwargs['eval_env']: <BSMarketEval instance>\n",
      "learn_kwargs['tb_log_name']: ddpg_220607-2232\n",
      "learn_kwargs['eval_log_path']: ../logs/tb_logs/ddpg_220607-2232_1\n"
     ]
    }
   ],
   "source": [
    "config.reconstruct_config(env_kwargs, model_kwargs, learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost': 0.02,\n",
      " 'dividend': 0.0,\n",
      " 'drift': 0.0,\n",
      " 'freq': 1,\n",
      " 'gen_name': 'gbm',\n",
      " 'init_price': 1.0,\n",
      " 'n_assets': 1000,\n",
      " 'n_periods': 30,\n",
      " 'payoff': 'european',\n",
      " 'period_unit': 365,\n",
      " 'reward_mode': 'pnl',\n",
      " 'risk_free_interest': 0.0,\n",
      " 'strike': 1.0,\n",
      " 'volatility': 0.2}\n"
     ]
    }
   ],
   "source": [
    "pprint(env_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_noise': NormalActionNoise(mu=0.0, sigma=0.1),\n",
      " 'batch_size': 15,\n",
      " 'buffer_size': 300,\n",
      " 'create_eval_env': False,\n",
      " 'device': 'auto',\n",
      " 'env': <Env.env.BSMarket object at 0x0000028D7C8A8910>,\n",
      " 'gamma': 0.99,\n",
      " 'gradient_steps': -1,\n",
      " 'learning_rate': <function lr_schedule at 0x0000028CAD210280>,\n",
      " 'learning_starts': 300,\n",
      " 'optimize_memory_usage': False,\n",
      " 'policy': <class 'Algorithms.ddpg.policies_legacy.DoubleDDPGPolicy'>,\n",
      " 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                   'actor_net_kwargs': None,\n",
      "                   'critic_net_kwargs': None,\n",
      "                   'features_extractor_class': <class 'Env.feature_extractor.MarketObsExtractor'>,\n",
      "                   'features_extractor_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                                                 'features_in': 4,\n",
      "                                                 'features_out': 2,\n",
      "                                                 'last_activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                                                 'net_arch': [32, 64]},\n",
      "                   'n_critics': 1,\n",
      "                   'net_arch': [],\n",
      "                   'normalize_images': False,\n",
      "                   'ntb_mode': False,\n",
      "                   'optimizer_class': <class 'torch.optim.adam.Adam'>,\n",
      "                   'optimizer_kwargs': None,\n",
      "                   'share_features_extractor': True},\n",
      " 'replay_buffer_class': <class 'Env.buffers.CustomReplayBuffer'>,\n",
      " 'replay_buffer_kwargs': {},\n",
      " 'seed': 42,\n",
      " 'std_coeff': 0.02,\n",
      " 'tau': 0.005,\n",
      " 'tensorboard_log': '../logs/tb_logs',\n",
      " 'train_freq': (1, 'episode'),\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "pprint(model_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'callback': <Algorithms.ddpg.callbacks.ReportCallbacks object at 0x0000028D7C897490>,\n",
      " 'eval_env': <Env.env.BSMarketEval object at 0x0000028D4A7BBD90>,\n",
      " 'eval_freq': 30,\n",
      " 'eval_log_path': '../logs/tb_logs/ddpg_220607-2232_1',\n",
      " 'log_interval': 30,\n",
      " 'n_eval_episodes': 1,\n",
      " 'reset_num_timesteps': True,\n",
      " 'tb_log_name': 'ddpg_220607-2232',\n",
      " 'total_timesteps': 1500}\n"
     ]
    }
   ],
   "source": [
    "pprint(learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Make env, model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from Algorithms.ddpg import DDPG\n",
    "# from Algorithms.ddpg.double_ddpg import DDPG\n",
    "\n",
    "model = DDPG(**model_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "DoubleDDPGPolicy(\n  (actor): CustomActor(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (mu): Sequential(\n      (0): Linear(in_features=2, out_features=1, bias=True)\n      (1): Tanh()\n    )\n    (flatten): Flatten(start_dim=-2, end_dim=-1)\n  )\n  (actor_target): CustomActor(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (mu): Sequential(\n      (0): Linear(in_features=2, out_features=1, bias=True)\n      (1): Tanh()\n    )\n    (flatten): Flatten(start_dim=-2, end_dim=-1)\n  )\n  (critic): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n  (critic2): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n  (critic_target): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n  (critic2_target): CustomContinuousCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ../logs/tb_logs\\ddpg_220607-2214_1\n",
      "[Training Start]\n",
      "Eval num_timesteps=30, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.021   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 30       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0222  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0235  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0243  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 120      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0226  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 150      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0255  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0223  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 210      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0204  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 240      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0218  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 270      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0209  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 300      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0251  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 330      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00211 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 360      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.68     |\n",
      "|    critic2_loss    | 3.01     |\n",
      "|    critic_loss     | 3.24     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    mean_cost_loss  | -1.65    |\n",
      "|    n_updates       | 30       |\n",
      "|    std_cost_loss   | 0.029    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000249 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 390      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.31     |\n",
      "|    critic2_loss    | 5.29     |\n",
      "|    critic_loss     | 5.43     |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    mean_cost_loss  | -2.27    |\n",
      "|    n_updates       | 60       |\n",
      "|    std_cost_loss   | 0.0389   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=420, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 30        |\n",
      "|    mean_reward     | -0.000189 |\n",
      "| time/              |           |\n",
      "|    total timesteps | 420       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.05      |\n",
      "|    critic2_loss    | 4.42      |\n",
      "|    critic_loss     | 4.26      |\n",
      "|    learning_rate   | 0.00176   |\n",
      "|    mean_cost_loss  | -2.02     |\n",
      "|    n_updates       | 90        |\n",
      "|    std_cost_loss   | 0.0309    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=450, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000413 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 450      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.65     |\n",
      "|    critic2_loss    | 3.11     |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    mean_cost_loss  | -1.63    |\n",
      "|    n_updates       | 120      |\n",
      "|    std_cost_loss   | 0.0175   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00159  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 480      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.26     |\n",
      "|    critic2_loss    | 2        |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    mean_cost_loss  | -1.25    |\n",
      "|    n_updates       | 150      |\n",
      "|    std_cost_loss   | 0.0123   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=510, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000497 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 510      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.938    |\n",
      "|    critic2_loss    | 1.2      |\n",
      "|    critic_loss     | 0.975    |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    mean_cost_loss  | -0.916   |\n",
      "|    n_updates       | 180      |\n",
      "|    std_cost_loss   | 0.0226   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000697 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 540      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.664    |\n",
      "|    critic2_loss    | 0.677    |\n",
      "|    critic_loss     | 0.54     |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    mean_cost_loss  | -0.637   |\n",
      "|    n_updates       | 210      |\n",
      "|    std_cost_loss   | 0.027    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00103  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 570      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.45     |\n",
      "|    critic2_loss    | 0.353    |\n",
      "|    critic_loss     | 0.289    |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    mean_cost_loss  | -0.42    |\n",
      "|    n_updates       | 240      |\n",
      "|    std_cost_loss   | 0.0293   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 1e-05    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.291    |\n",
      "|    critic2_loss    | 0.168    |\n",
      "|    critic_loss     | 0.151    |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    mean_cost_loss  | -0.261   |\n",
      "|    n_updates       | 270      |\n",
      "|    std_cost_loss   | 0.0305   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00206  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 630      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.187    |\n",
      "|    critic2_loss    | 0.0565   |\n",
      "|    critic_loss     | 0.0619   |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    mean_cost_loss  | -0.155   |\n",
      "|    n_updates       | 300      |\n",
      "|    std_cost_loss   | 0.0312   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=660, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000401 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 660      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.124    |\n",
      "|    critic2_loss    | 0.00871  |\n",
      "|    critic_loss     | 0.0214   |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    mean_cost_loss  | -0.0922  |\n",
      "|    n_updates       | 330      |\n",
      "|    std_cost_loss   | 0.0316   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00191  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 690      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0822   |\n",
      "|    critic2_loss    | 0.00326  |\n",
      "|    critic_loss     | 0.0215   |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    mean_cost_loss  | -0.0502  |\n",
      "|    n_updates       | 360      |\n",
      "|    std_cost_loss   | 0.032    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00139  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 720      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0486   |\n",
      "|    critic2_loss    | 0.0011   |\n",
      "|    critic_loss     | 0.0222   |\n",
      "|    learning_rate   | 0.000979 |\n",
      "|    mean_cost_loss  | -0.0164  |\n",
      "|    n_updates       | 390      |\n",
      "|    std_cost_loss   | 0.0322   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00056  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 750      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0204   |\n",
      "|    critic2_loss    | 0.00133  |\n",
      "|    critic_loss     | 0.0256   |\n",
      "|    learning_rate   | 0.000932 |\n",
      "|    mean_cost_loss  | 0.012    |\n",
      "|    n_updates       | 420      |\n",
      "|    std_cost_loss   | 0.0325   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 30        |\n",
      "|    mean_reward     | -0.000481 |\n",
      "| time/              |           |\n",
      "|    total timesteps | 780       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.00546  |\n",
      "|    critic2_loss    | 0.00264   |\n",
      "|    critic_loss     | 0.0301    |\n",
      "|    learning_rate   | 0.000889  |\n",
      "|    mean_cost_loss  | 0.0381    |\n",
      "|    n_updates       | 450       |\n",
      "|    std_cost_loss   | 0.0326    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=810, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00327  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 810      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0282  |\n",
      "|    critic2_loss    | 0.00588  |\n",
      "|    critic_loss     | 0.0299   |\n",
      "|    learning_rate   | 0.00085  |\n",
      "|    mean_cost_loss  | 0.061    |\n",
      "|    n_updates       | 480      |\n",
      "|    std_cost_loss   | 0.0328   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=840, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00129  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 840      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0463  |\n",
      "|    critic2_loss    | 0.00887  |\n",
      "|    critic_loss     | 0.0343   |\n",
      "|    learning_rate   | 0.000814 |\n",
      "|    mean_cost_loss  | 0.0792   |\n",
      "|    n_updates       | 510      |\n",
      "|    std_cost_loss   | 0.0329   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00134  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 870      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0632  |\n",
      "|    critic2_loss    | 0.0115   |\n",
      "|    critic_loss     | 0.0376   |\n",
      "|    learning_rate   | 0.000781 |\n",
      "|    mean_cost_loss  | 0.0962   |\n",
      "|    n_updates       | 540      |\n",
      "|    std_cost_loss   | 0.0331   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 30        |\n",
      "|    mean_reward     | -0.000404 |\n",
      "| time/              |           |\n",
      "|    total timesteps | 900       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.0793   |\n",
      "|    critic2_loss    | 0.0151    |\n",
      "|    critic_loss     | 0.0434    |\n",
      "|    learning_rate   | 0.000751  |\n",
      "|    mean_cost_loss  | 0.112     |\n",
      "|    n_updates       | 570       |\n",
      "|    std_cost_loss   | 0.0332    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.0846  |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 20       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total timesteps | 900      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000553 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 930      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0943  |\n",
      "|    critic2_loss    | 0.0196   |\n",
      "|    critic_loss     | 0.0442   |\n",
      "|    learning_rate   | 0.000723 |\n",
      "|    mean_cost_loss  | 0.128    |\n",
      "|    n_updates       | 600      |\n",
      "|    std_cost_loss   | 0.0333   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00312  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 960      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.106   |\n",
      "|    critic2_loss    | 0.0219   |\n",
      "|    critic_loss     | 0.0491   |\n",
      "|    learning_rate   | 0.000697 |\n",
      "|    mean_cost_loss  | 0.14     |\n",
      "|    n_updates       | 630      |\n",
      "|    std_cost_loss   | 0.0334   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00176  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 990      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.119   |\n",
      "|    critic2_loss    | 0.026    |\n",
      "|    critic_loss     | 0.0515   |\n",
      "|    learning_rate   | 0.000674 |\n",
      "|    mean_cost_loss  | 0.152    |\n",
      "|    n_updates       | 660      |\n",
      "|    std_cost_loss   | 0.0335   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 30        |\n",
      "|    mean_reward     | -0.000319 |\n",
      "| time/              |           |\n",
      "|    total timesteps | 1020      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.13     |\n",
      "|    critic2_loss    | 0.031     |\n",
      "|    critic_loss     | 0.0519    |\n",
      "|    learning_rate   | 0.000652  |\n",
      "|    mean_cost_loss  | 0.163     |\n",
      "|    n_updates       | 690       |\n",
      "|    std_cost_loss   | 0.0336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1050, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000408 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.138   |\n",
      "|    critic2_loss    | 0.0334   |\n",
      "|    critic_loss     | 0.0553   |\n",
      "|    learning_rate   | 0.000633 |\n",
      "|    mean_cost_loss  | 0.172    |\n",
      "|    n_updates       | 720      |\n",
      "|    std_cost_loss   | 0.0336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00278  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.145   |\n",
      "|    critic2_loss    | 0.0365   |\n",
      "|    critic_loss     | 0.0569   |\n",
      "|    learning_rate   | 0.000615 |\n",
      "|    mean_cost_loss  | 0.179    |\n",
      "|    n_updates       | 750      |\n",
      "|    std_cost_loss   | 0.0337   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00125  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.152   |\n",
      "|    critic2_loss    | 0.0398   |\n",
      "|    critic_loss     | 0.0591   |\n",
      "|    learning_rate   | 0.000599 |\n",
      "|    mean_cost_loss  | 0.185    |\n",
      "|    n_updates       | 780      |\n",
      "|    std_cost_loss   | 0.0338   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00228  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.158   |\n",
      "|    critic2_loss    | 0.0416   |\n",
      "|    critic_loss     | 0.0622   |\n",
      "|    learning_rate   | 0.000584 |\n",
      "|    mean_cost_loss  | 0.192    |\n",
      "|    n_updates       | 810      |\n",
      "|    std_cost_loss   | 0.0339   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00226  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.164   |\n",
      "|    critic2_loss    | 0.0457   |\n",
      "|    critic_loss     | 0.0612   |\n",
      "|    learning_rate   | 0.000571 |\n",
      "|    mean_cost_loss  | 0.197    |\n",
      "|    n_updates       | 840      |\n",
      "|    std_cost_loss   | 0.0339   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00134  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.167   |\n",
      "|    critic2_loss    | 0.0455   |\n",
      "|    critic_loss     | 0.0669   |\n",
      "|    learning_rate   | 0.000559 |\n",
      "|    mean_cost_loss  | 0.201    |\n",
      "|    n_updates       | 870      |\n",
      "|    std_cost_loss   | 0.034    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000405 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.173   |\n",
      "|    critic2_loss    | 0.049    |\n",
      "|    critic_loss     | 0.0664   |\n",
      "|    learning_rate   | 0.000548 |\n",
      "|    mean_cost_loss  | 0.207    |\n",
      "|    n_updates       | 900      |\n",
      "|    std_cost_loss   | 0.034    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00033  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.177   |\n",
      "|    critic2_loss    | 0.0509   |\n",
      "|    critic_loss     | 0.0668   |\n",
      "|    learning_rate   | 0.000539 |\n",
      "|    mean_cost_loss  | 0.211    |\n",
      "|    n_updates       | 930      |\n",
      "|    std_cost_loss   | 0.0341   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00131  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.181   |\n",
      "|    critic2_loss    | 0.0525   |\n",
      "|    critic_loss     | 0.0692   |\n",
      "|    learning_rate   | 0.00053  |\n",
      "|    mean_cost_loss  | 0.215    |\n",
      "|    n_updates       | 960      |\n",
      "|    std_cost_loss   | 0.0342   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00169  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.185   |\n",
      "|    critic2_loss    | 0.0542   |\n",
      "|    critic_loss     | 0.0692   |\n",
      "|    learning_rate   | 0.000523 |\n",
      "|    mean_cost_loss  | 0.219    |\n",
      "|    n_updates       | 990      |\n",
      "|    std_cost_loss   | 0.0342   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00089  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.187   |\n",
      "|    critic2_loss    | 0.0548   |\n",
      "|    critic_loss     | 0.0709   |\n",
      "|    learning_rate   | 0.000517 |\n",
      "|    mean_cost_loss  | 0.221    |\n",
      "|    n_updates       | 1020     |\n",
      "|    std_cost_loss   | 0.0343   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000646 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.19    |\n",
      "|    critic2_loss    | 0.0565   |\n",
      "|    critic_loss     | 0.0734   |\n",
      "|    learning_rate   | 0.000512 |\n",
      "|    mean_cost_loss  | 0.224    |\n",
      "|    n_updates       | 1050     |\n",
      "|    std_cost_loss   | 0.0343   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00284  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.195   |\n",
      "|    critic2_loss    | 0.0588   |\n",
      "|    critic_loss     | 0.0723   |\n",
      "|    learning_rate   | 0.000507 |\n",
      "|    mean_cost_loss  | 0.229    |\n",
      "|    n_updates       | 1080     |\n",
      "|    std_cost_loss   | 0.0344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000855 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.197   |\n",
      "|    critic2_loss    | 0.058    |\n",
      "|    critic_loss     | 0.0751   |\n",
      "|    learning_rate   | 0.000504 |\n",
      "|    mean_cost_loss  | 0.231    |\n",
      "|    n_updates       | 1110     |\n",
      "|    std_cost_loss   | 0.0344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.000574 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.2     |\n",
      "|    critic2_loss    | 0.0599   |\n",
      "|    critic_loss     | 0.074    |\n",
      "|    learning_rate   | 0.000502 |\n",
      "|    mean_cost_loss  | 0.234    |\n",
      "|    n_updates       | 1140     |\n",
      "|    std_cost_loss   | 0.0345   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00133  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.201   |\n",
      "|    critic2_loss    | 0.0616   |\n",
      "|    critic_loss     | 0.0733   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    mean_cost_loss  | 0.236    |\n",
      "|    n_updates       | 1170     |\n",
      "|    std_cost_loss   | 0.0345   |\n",
      "---------------------------------\n",
      "[Training End]  steps: 1500\ttimes: 100.2240834236145\n"
     ]
    }
   ],
   "source": [
    "model = model.learn(**learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BSMarket instance> will be save as name. env_kwargs not in kwargs!\n",
      "<BSMarketEval instance> will be save as name. eval_env_kwargs not in kwargs!\n",
      "<Algorithms.ddpg.callbacks.ReportCallbacks object at 0x0000028CAD33CFA0> will be save as name. callback_kwargs not in kwargs!\n",
      "../logs/tb_logs/ddpg_220607-2214_1/config.yaml was saved.\n"
     ]
    }
   ],
   "source": [
    "config.save_config(f'{learn_kwargs[\"eval_log_path\"]}/config.yaml', env_kwargs, model_kwargs, learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. P&L Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "model = model.load('../logs/tb_logs/ddpg_220607-2214_1'+'/best_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<Env.env.BSMarketEval at 0x28d4a7bbd90>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env = learn_kwargs['eval_env']\n",
    "eval_env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "random_pnl = eval_env.pnl_eval()\n",
    "delta_pnl = eval_env.delta_eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.21054590340761983"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pnl.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.029680267234441747"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_pnl.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "rl_pnl = eval_env.pnl_eval(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0011812028750596332"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_pnl.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pnl_reward(pnl):\n",
    "    mean = np.mean(pnl)\n",
    "    std = np.std(pnl)\n",
    "    return mean - 0.02 * std , (mean, std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random:\t-0.2119, -0.2105, 0.0680\n",
      "delta:\t\t-0.0309, -0.0297, 0.0632\n",
      "rl:\t\t0.0005, 0.0012, 0.0348\n",
      "5.848138259862849\n",
      "64.68934922494776\n"
     ]
    }
   ],
   "source": [
    "plt.xlabel('P&L')\n",
    "# plt.hist(random_pnl, bins=100, range=(-0.25, 0.05), alpha=0.6, label='random')\n",
    "plt.hist(random_pnl, bins=100, range=(-0.2, 0.05), alpha=0.6, label='random')\n",
    "plt.hist(delta_pnl, bins=100, range=(-0.2, 0.05), alpha=0.6, label='delta')\n",
    "plt.hist(rl_pnl, bins=100, range=(-0.2, 0.05), alpha=0.6, label='rl')\n",
    "plt.ylim(0, 150)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "r1, (m1, s1) = pnl_reward(random_pnl)\n",
    "r2, (m2, s2) = pnl_reward(delta_pnl)\n",
    "r3, (m3, s3) = pnl_reward(rl_pnl)\n",
    "print(f'random:\\t{r1:.4f}, {m1:.4f}, {s1:.4f}')\n",
    "print(f'delta:\\t\\t{r2:.4f}, {m2:.4f}, {s2:.4f}')\n",
    "print(f'rl:\\t\\t{r3:.4f}, {m3:.4f}, {s3:.4f}')\n",
    "print((r2-r1)/abs(r2))\n",
    "print((r3-r2)/abs(r3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}