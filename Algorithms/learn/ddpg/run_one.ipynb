{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from Algorithms.ddpg import config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Plot Setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib\n",
    "\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "FONTSIZE = 10\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "matplotlib.rcParams[\"figure.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"legend.fontsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "matplotlib.rcParams[\"savefig.pad_inches\"] = 0.1\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2\n",
    "matplotlib.rcParams[\"axes.linewidth\"] = 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Model Setting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Load Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env 'BSMarket was created!\n",
      "env 'BSMarket was created!\n"
     ]
    }
   ],
   "source": [
    "env_kwargs, model_kwargs, learn_kwargs = config.load_config('tmp_config.yaml')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "ntb_mode = False\n",
    "double_ddpg = True\n",
    "\n",
    "env_kwargs.update({\n",
    "    'n_assets': 1,\n",
    "    'reward_fn': 'mean var',\n",
    "    'reward_fn_kwargs': {},\n",
    "    'reward_mode': 'pnl'\n",
    "})\n",
    "\n",
    "model_kwargs.update({\n",
    "    'buffer_size': 100*100,\n",
    "    'learning_starts': 500,\n",
    "    'batch_size': 100,\n",
    "    'train_freq': (3, 'episode'),\n",
    "    'std_coeff': 0.05,\n",
    "    'gradient_steps': 30,\n",
    "})\n",
    "\n",
    "model_kwargs['policy_kwargs'].update({\n",
    "    'ntb_mode': ntb_mode,\n",
    "    'double_ddpg': double_ddpg,\n",
    "})\n",
    "\n",
    "learn_kwargs.update({\n",
    "    'total_timesteps': 100*50\n",
    "})\n",
    "\n",
    "# del model_kwargs['std_coeff']\n",
    "\n",
    "if ntb_mode:\n",
    "    features_out = 64\n",
    "    model_kwargs['policy_kwargs']['features_extractor_kwargs'].update({\n",
    "        'features_out': features_out,\n",
    "        'net_arch': [32]\n",
    "    })\n",
    "\n",
    "    actor_net_kwargs = {'bn_kwargs': {'num_features': features_out}}\n",
    "    critic_net_kwargs = {'bn_kwargs': {'num_features': features_out+1}}\n",
    "\n",
    "    model_kwargs['policy_kwargs'].update({\n",
    "        'net_arch': {'pi': [(nn.BatchNorm1d, 'bn'), 32, 32],\n",
    "                     'qf': [(nn.BatchNorm1d, 'bn'), 2]},\n",
    "        'actor_net_kwargs': actor_net_kwargs,\n",
    "        'critic_net_kwargs': critic_net_kwargs,\n",
    "    })\n",
    "\n",
    "else:\n",
    "    model_kwargs['policy_kwargs'].update({\n",
    "        'net_arch': [],\n",
    "    })\n",
    "\n",
    "    model_kwargs['policy_kwargs']['features_extractor_kwargs'].update({\n",
    "        'features_out': 2,\n",
    "        'net_arch': [32, 64]\n",
    "    })\n",
    "\n",
    "model_kwargs['policy_kwargs']['one_asset'] = (env_kwargs['n_assets']==1)\n",
    "if model_kwargs['policy_kwargs']['one_asset']:\n",
    "    model_kwargs['replay_buffer_class'] = None\n",
    "    model_kwargs['replay_buffer_kwargs'] = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env 'BSMarket was created!\n",
      "model_kwargs['env']: <BSMarket instance>\n",
      "env 'BSMarket was created!\n",
      "learn_kwargs['eval_env']: <BSMarketEval instance>\n",
      "learn_kwargs['tb_log_name']: ddpg_220616-0039\n",
      "learn_kwargs['eval_log_path']: ../logs/tb_logs/ddpg_220616-0039_1\n"
     ]
    }
   ],
   "source": [
    "config.reconstruct_config(env_kwargs, model_kwargs, learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost': 0.02,\n",
      " 'dividend': 0.0,\n",
      " 'drift': 0.0,\n",
      " 'freq': 1,\n",
      " 'gen_name': 'gbm',\n",
      " 'init_price': 1.0,\n",
      " 'maturity': 30,\n",
      " 'n_assets': 1,\n",
      " 'payoff': 'european',\n",
      " 'payoff_coeff': 1.0,\n",
      " 'period_unit': 365,\n",
      " 'reward_fn': 'mean var',\n",
      " 'reward_fn_kwargs': {},\n",
      " 'reward_mode': 'pnl',\n",
      " 'risk_free_interest': 0.0,\n",
      " 'strike': 1.0,\n",
      " 'volatility': 0.2}\n"
     ]
    }
   ],
   "source": [
    "pprint(env_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_noise': NormalActionNoise(mu=0.0, sigma=0.1),\n",
      " 'batch_size': 100,\n",
      " 'buffer_size': 10000,\n",
      " 'create_eval_env': False,\n",
      " 'device': 'auto',\n",
      " 'env': <Env.env.BSMarket object at 0x000001E644DD7C10>,\n",
      " 'gamma': 0.99,\n",
      " 'gradient_steps': 30,\n",
      " 'learning_rate': <function lr_schedule at 0x000001E5ECF97CA0>,\n",
      " 'learning_starts': 500,\n",
      " 'optimize_memory_usage': False,\n",
      " 'policy': <class 'Algorithms.ddpg.policies.DoubleDDPGPolicy'>,\n",
      " 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                   'actor_net_kwargs': None,\n",
      "                   'critic_net_kwargs': None,\n",
      "                   'double_ddpg': True,\n",
      "                   'features_extractor_class': <class 'Env.feature_extractor.MarketObsExtractor'>,\n",
      "                   'features_extractor_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                                                 'features_in': 4,\n",
      "                                                 'features_out': 2,\n",
      "                                                 'last_activation_fn': <class 'torch.nn.modules.activation.ReLU'>,\n",
      "                                                 'net_arch': [32, 64]},\n",
      "                   'n_critics': 1,\n",
      "                   'net_arch': [],\n",
      "                   'normalize_images': False,\n",
      "                   'ntb_mode': False,\n",
      "                   'one_asset': True,\n",
      "                   'optimizer_class': <class 'torch.optim.adam.Adam'>,\n",
      "                   'optimizer_kwargs': None,\n",
      "                   'share_features_extractor': True},\n",
      " 'replay_buffer_class': None,\n",
      " 'replay_buffer_kwargs': None,\n",
      " 'seed': 42,\n",
      " 'std_coeff': 0.05,\n",
      " 'tau': 0.005,\n",
      " 'tensorboard_log': '../logs/tb_logs',\n",
      " 'train_freq': (3, 'episode'),\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "pprint(model_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'callback': <Algorithms.ddpg.callbacks.ReportCallbacks object at 0x000001E5ED0F5E50>,\n",
      " 'eval_env': <Env.env.BSMarketEval object at 0x000001E644DD7DC0>,\n",
      " 'eval_freq': 30,\n",
      " 'eval_log_path': '../logs/tb_logs/ddpg_220616-0039_1',\n",
      " 'log_interval': 30,\n",
      " 'n_eval_episodes': 1,\n",
      " 'reset_num_timesteps': True,\n",
      " 'tb_log_name': 'ddpg_220616-0039',\n",
      " 'total_timesteps': 5000}\n"
     ]
    }
   ],
   "source": [
    "pprint(learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "pnl\n",
      "<function mean_variance_reward at 0x000001E5D4F17C10>\n"
     ]
    }
   ],
   "source": [
    "print(learn_kwargs['eval_env'].n_assets)\n",
    "print(learn_kwargs['eval_env'].reward_mode)\n",
    "print(learn_kwargs['eval_env'].reward_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Make env, model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from Algorithms.ddpg import DoubleDDPG\n",
    "from Algorithms.ddpg.double_ddpg import QDDPG\n",
    "from stable_baselines3.ddpg import DDPG\n",
    "# from Algorithms.ddpg.double_ddpg import DDPG\n",
    "\n",
    "# model = DDPG(**model_kwargs)\n",
    "model = QDDPG(**model_kwargs)\n",
    "# model = DoubleDDPG(**model_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "DoubleDDPGPolicy(\n  (actor): FlattenActor(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (mu): Sequential(\n      (0): Linear(in_features=2, out_features=1, bias=True)\n      (1): Tanh()\n    )\n  )\n  (actor_target): FlattenActor(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (mu): Sequential(\n      (0): Linear(in_features=2, out_features=1, bias=True)\n      (1): Tanh()\n    )\n  )\n  (critic): FlattenCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n  (critic_target): FlattenCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n  (critic2): FlattenCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n  (critic2_target): FlattenCritic(\n    (features_extractor): MarketObsExtractor(\n      (layers): Sequential(\n        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=4, out_features=32, bias=True)\n        (2): ReLU()\n        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (4): Linear(in_features=32, out_features=64, bias=True)\n        (5): ReLU()\n        (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (7): Linear(in_features=64, out_features=2, bias=True)\n        (8): ReLU()\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=3, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ../logs/tb_logs\\ddpg_220616-0039_1\n",
      "[Training Start]\n",
      "Eval num_timesteps=30, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0231  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 30       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0292  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0214  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 90       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00843 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 120      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00205 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 150      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0203  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 210      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0453  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 240      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0484  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 270      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00627 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 300      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0258  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 330      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0616  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 360      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 390      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0184  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 420      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0171  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 450      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0179  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 480      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00148  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 510      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=540, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0046  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 540      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.158   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 570      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.624    |\n",
      "|    critic2_loss    | 0.295    |\n",
      "|    critic_loss     | 0.835    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    mean_cost_loss  | -0.593   |\n",
      "|    n_updates       | 30       |\n",
      "|    std_cost_loss   | 0.031    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.189   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 600      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630, episode_reward=-0.14 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.141   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 630      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0567  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 660      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.01     |\n",
      "|    critic2_loss    | 0.28     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    mean_cost_loss  | -0.961   |\n",
      "|    n_updates       | 60       |\n",
      "|    std_cost_loss   | 0.0457   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0825  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 690      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.177   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 720      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0267  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 750      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.678    |\n",
      "|    critic2_loss    | 0.0452   |\n",
      "|    critic_loss     | 0.471    |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    mean_cost_loss  | -0.66    |\n",
      "|    n_updates       | 90       |\n",
      "|    std_cost_loss   | 0.0184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00759 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 780      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0402  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 810      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.179   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 840      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.449    |\n",
      "|    critic2_loss    | 0.0231   |\n",
      "|    critic_loss     | 0.121    |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    mean_cost_loss  | -0.433   |\n",
      "|    n_updates       | 120      |\n",
      "|    std_cost_loss   | 0.0154   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.204   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 870      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0189  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 900      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.162   |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 115      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total timesteps | 900      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.031   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 930      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.506    |\n",
      "|    critic2_loss    | 0.021    |\n",
      "|    critic_loss     | 0.116    |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    mean_cost_loss  | -0.492   |\n",
      "|    n_updates       | 150      |\n",
      "|    std_cost_loss   | 0.0137   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0345  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 960      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00317  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 990      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1020, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.178   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.587    |\n",
      "|    critic2_loss    | 0.0205   |\n",
      "|    critic_loss     | 0.121    |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    mean_cost_loss  | -0.565   |\n",
      "|    n_updates       | 180      |\n",
      "|    std_cost_loss   | 0.0218   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0339  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.00845 |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0439  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.695    |\n",
      "|    critic2_loss    | 0.029    |\n",
      "|    critic_loss     | 0.156    |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    mean_cost_loss  | -0.665   |\n",
      "|    n_updates       | 210      |\n",
      "|    std_cost_loss   | 0.0304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00126  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.0034   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1170     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1200, episode_reward=-0.09 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0876  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.861    |\n",
      "|    critic2_loss    | 0.0278   |\n",
      "|    critic_loss     | 0.149    |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    mean_cost_loss  | -0.819   |\n",
      "|    n_updates       | 240      |\n",
      "|    std_cost_loss   | 0.0421   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0455  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0197  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0675  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.03     |\n",
      "|    critic2_loss    | 0.0303   |\n",
      "|    critic_loss     | 0.167    |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    mean_cost_loss  | -0.975   |\n",
      "|    n_updates       | 270      |\n",
      "|    std_cost_loss   | 0.051    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0301  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00606  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1350     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1380, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 0.00467  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.05     |\n",
      "|    critic2_loss    | 0.0297   |\n",
      "|    critic_loss     | 0.154    |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    mean_cost_loss  | -0.993   |\n",
      "|    n_updates       | 300      |\n",
      "|    std_cost_loss   | 0.0524   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0223  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0101  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.105   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.03     |\n",
      "|    critic2_loss    | 0.0295   |\n",
      "|    critic_loss     | 0.159    |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    mean_cost_loss  | -0.976   |\n",
      "|    n_updates       | 330      |\n",
      "|    std_cost_loss   | 0.0517   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0627  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.02     |\n",
      "|    critic2_loss    | 0.0284   |\n",
      "|    critic_loss     | 0.157    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    mean_cost_loss  | -0.968   |\n",
      "|    n_updates       | 360      |\n",
      "|    std_cost_loss   | 0.0514   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0316  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0532  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0257  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.02     |\n",
      "|    critic2_loss    | 0.028    |\n",
      "|    critic_loss     | 0.162    |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    mean_cost_loss  | -0.971   |\n",
      "|    n_updates       | 390      |\n",
      "|    std_cost_loss   | 0.0515   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.02    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.025   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0345  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1        |\n",
      "|    critic2_loss    | 0.0343   |\n",
      "|    critic_loss     | 0.186    |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    mean_cost_loss  | -0.95    |\n",
      "|    n_updates       | 420      |\n",
      "|    std_cost_loss   | 0.0502   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.1     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0494  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.106   |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total timesteps | 1800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0456  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.942    |\n",
      "|    critic2_loss    | 0.0421   |\n",
      "|    critic_loss     | 0.23     |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    mean_cost_loss  | -0.895   |\n",
      "|    n_updates       | 450      |\n",
      "|    std_cost_loss   | 0.0469   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0397  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890, episode_reward=-0.09 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0931  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920, episode_reward=-0.12 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.117   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.846    |\n",
      "|    critic2_loss    | 0.103    |\n",
      "|    critic_loss     | 0.373    |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    mean_cost_loss  | -0.805   |\n",
      "|    n_updates       | 480      |\n",
      "|    std_cost_loss   | 0.0411   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0986  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980, episode_reward=-0.14 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.136   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.329   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.629    |\n",
      "|    critic2_loss    | 0.306    |\n",
      "|    critic_loss     | 0.719    |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    mean_cost_loss  | -0.605   |\n",
      "|    n_updates       | 510      |\n",
      "|    std_cost_loss   | 0.0241   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040, episode_reward=-0.60 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.601   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070, episode_reward=-0.23 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.23    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100, episode_reward=-0.59 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.587   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.371    |\n",
      "|    critic2_loss    | 0.169    |\n",
      "|    critic_loss     | 0.684    |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    mean_cost_loss  | -0.352   |\n",
      "|    n_updates       | 540      |\n",
      "|    std_cost_loss   | 0.0184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130, episode_reward=-0.54 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.542   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160, episode_reward=-0.50 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.503   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190, episode_reward=-0.58 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.578   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.346    |\n",
      "|    critic2_loss    | 0.0518   |\n",
      "|    critic_loss     | 0.267    |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    mean_cost_loss  | -0.328   |\n",
      "|    n_updates       | 570      |\n",
      "|    std_cost_loss   | 0.0178   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220, episode_reward=-0.67 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.67    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250, episode_reward=-0.54 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.539   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280, episode_reward=-0.57 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.57    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.381    |\n",
      "|    critic2_loss    | 0.0345   |\n",
      "|    critic_loss     | 0.131    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    mean_cost_loss  | -0.369   |\n",
      "|    n_updates       | 600      |\n",
      "|    std_cost_loss   | 0.0121   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310, episode_reward=-0.61 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.605   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.531   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370, episode_reward=-0.43 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.43    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.29     |\n",
      "|    critic2_loss    | 0.0245   |\n",
      "|    critic_loss     | 0.105    |\n",
      "|    learning_rate   | 0.000959 |\n",
      "|    mean_cost_loss  | -0.27    |\n",
      "|    n_updates       | 630      |\n",
      "|    std_cost_loss   | 0.0205   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.372   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430, episode_reward=-0.60 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.605   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460, episode_reward=-0.24 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.241   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.141    |\n",
      "|    critic2_loss    | 0.0483   |\n",
      "|    critic_loss     | 0.138    |\n",
      "|    learning_rate   | 0.000919 |\n",
      "|    mean_cost_loss  | -0.114   |\n",
      "|    n_updates       | 660      |\n",
      "|    std_cost_loss   | 0.0271   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490, episode_reward=-0.27 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.27    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520, episode_reward=-0.25 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.248   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.152   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0128  |\n",
      "|    critic2_loss    | 0.0584   |\n",
      "|    critic_loss     | 0.166    |\n",
      "|    learning_rate   | 0.000881 |\n",
      "|    mean_cost_loss  | 0.0456   |\n",
      "|    n_updates       | 690      |\n",
      "|    std_cost_loss   | 0.0328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.166   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.173   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.153   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.16    |\n",
      "|    critic2_loss    | 0.0266   |\n",
      "|    critic_loss     | 0.0717   |\n",
      "|    learning_rate   | 0.000846 |\n",
      "|    mean_cost_loss  | 0.196    |\n",
      "|    n_updates       | 720      |\n",
      "|    std_cost_loss   | 0.0359   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670, episode_reward=-0.14 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.14    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.149   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2700     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.189   |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total timesteps | 2700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.194   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.25    |\n",
      "|    critic2_loss    | 0.0191   |\n",
      "|    critic_loss     | 0.0287   |\n",
      "|    learning_rate   | 0.000814 |\n",
      "|    mean_cost_loss  | 0.286    |\n",
      "|    n_updates       | 750      |\n",
      "|    std_cost_loss   | 0.0362   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.101   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.159   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820, episode_reward=-0.09 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0869  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.298   |\n",
      "|    critic2_loss    | 0.0188   |\n",
      "|    critic_loss     | 0.0149   |\n",
      "|    learning_rate   | 0.000784 |\n",
      "|    mean_cost_loss  | 0.334    |\n",
      "|    n_updates       | 780      |\n",
      "|    std_cost_loss   | 0.0359   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850, episode_reward=-0.12 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.122   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.125   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0816  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.318   |\n",
      "|    critic2_loss    | 0.019    |\n",
      "|    critic_loss     | 0.0107   |\n",
      "|    learning_rate   | 0.000756 |\n",
      "|    mean_cost_loss  | 0.354    |\n",
      "|    n_updates       | 810      |\n",
      "|    std_cost_loss   | 0.0355   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.131   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0637  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.339   |\n",
      "|    critic2_loss    | 0.0195   |\n",
      "|    critic_loss     | 0.00858  |\n",
      "|    learning_rate   | 0.000731 |\n",
      "|    mean_cost_loss  | 0.375    |\n",
      "|    n_updates       | 840      |\n",
      "|    std_cost_loss   | 0.0356   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0726  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.066   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.114   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.343   |\n",
      "|    critic2_loss    | 0.0186   |\n",
      "|    critic_loss     | 0.00806  |\n",
      "|    learning_rate   | 0.000707 |\n",
      "|    mean_cost_loss  | 0.379    |\n",
      "|    n_updates       | 870      |\n",
      "|    std_cost_loss   | 0.0354   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3120, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0651  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0642  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.187   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.342   |\n",
      "|    critic2_loss    | 0.0203   |\n",
      "|    critic_loss     | 0.00868  |\n",
      "|    learning_rate   | 0.000685 |\n",
      "|    mean_cost_loss  | 0.378    |\n",
      "|    n_updates       | 900      |\n",
      "|    std_cost_loss   | 0.0352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0429  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0825  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0959  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.331   |\n",
      "|    critic2_loss    | 0.0187   |\n",
      "|    critic_loss     | 0.00855  |\n",
      "|    learning_rate   | 0.000665 |\n",
      "|    mean_cost_loss  | 0.366    |\n",
      "|    n_updates       | 930      |\n",
      "|    std_cost_loss   | 0.0349   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0387  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3330, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0833  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0346  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.353   |\n",
      "|    critic2_loss    | 0.0194   |\n",
      "|    critic_loss     | 0.00847  |\n",
      "|    learning_rate   | 0.000646 |\n",
      "|    mean_cost_loss  | 0.388    |\n",
      "|    n_updates       | 960      |\n",
      "|    std_cost_loss   | 0.0353   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0544  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0761  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3450, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0327  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.353   |\n",
      "|    critic2_loss    | 0.0163   |\n",
      "|    critic_loss     | 0.00672  |\n",
      "|    learning_rate   | 0.000629 |\n",
      "|    mean_cost_loss  | 0.388    |\n",
      "|    n_updates       | 990      |\n",
      "|    std_cost_loss   | 0.0351   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.102   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.126   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0626  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.344   |\n",
      "|    critic2_loss    | 0.0181   |\n",
      "|    critic_loss     | 0.008    |\n",
      "|    learning_rate   | 0.000613 |\n",
      "|    mean_cost_loss  | 0.379    |\n",
      "|    n_updates       | 1020     |\n",
      "|    std_cost_loss   | 0.0349   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570, episode_reward=-0.14 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.138   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3600, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.08    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.168   |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total timesteps | 3600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0693  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.334   |\n",
      "|    critic2_loss    | 0.0156   |\n",
      "|    critic_loss     | 0.00722  |\n",
      "|    learning_rate   | 0.000599 |\n",
      "|    mean_cost_loss  | 0.369    |\n",
      "|    n_updates       | 1050     |\n",
      "|    std_cost_loss   | 0.0348   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0668  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.08    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.128   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.343   |\n",
      "|    critic2_loss    | 0.017    |\n",
      "|    critic_loss     | 0.00749  |\n",
      "|    learning_rate   | 0.000586 |\n",
      "|    mean_cost_loss  | 0.378    |\n",
      "|    n_updates       | 1080     |\n",
      "|    std_cost_loss   | 0.0349   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0785  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.159   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810, episode_reward=-0.14 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.14    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.332   |\n",
      "|    critic2_loss    | 0.017    |\n",
      "|    critic_loss     | 0.00788  |\n",
      "|    learning_rate   | 0.000573 |\n",
      "|    mean_cost_loss  | 0.366    |\n",
      "|    n_updates       | 1110     |\n",
      "|    std_cost_loss   | 0.0346   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0373  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.153   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0844  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.333   |\n",
      "|    critic2_loss    | 0.017    |\n",
      "|    critic_loss     | 0.008    |\n",
      "|    learning_rate   | 0.000562 |\n",
      "|    mean_cost_loss  | 0.367    |\n",
      "|    n_updates       | 1140     |\n",
      "|    std_cost_loss   | 0.0346   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0836  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0593  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0437  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.341   |\n",
      "|    critic2_loss    | 0.0156   |\n",
      "|    critic_loss     | 0.00729  |\n",
      "|    learning_rate   | 0.000552 |\n",
      "|    mean_cost_loss  | 0.376    |\n",
      "|    n_updates       | 1170     |\n",
      "|    std_cost_loss   | 0.0346   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0697  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.147   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0845  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.344   |\n",
      "|    critic2_loss    | 0.0169   |\n",
      "|    critic_loss     | 0.00769  |\n",
      "|    learning_rate   | 0.000543 |\n",
      "|    mean_cost_loss  | 0.378    |\n",
      "|    n_updates       | 1200     |\n",
      "|    std_cost_loss   | 0.0346   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0415  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140, episode_reward=-0.12 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.124   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0334  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.341   |\n",
      "|    critic2_loss    | 0.0154   |\n",
      "|    critic_loss     | 0.00743  |\n",
      "|    learning_rate   | 0.000535 |\n",
      "|    mean_cost_loss  | 0.375    |\n",
      "|    n_updates       | 1230     |\n",
      "|    std_cost_loss   | 0.0343   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0522  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0471  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.324   |\n",
      "|    critic2_loss    | 0.0135   |\n",
      "|    critic_loss     | 0.00657  |\n",
      "|    learning_rate   | 0.000528 |\n",
      "|    mean_cost_loss  | 0.358    |\n",
      "|    n_updates       | 1260     |\n",
      "|    std_cost_loss   | 0.0341   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0612  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.108   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350, episode_reward=-0.09 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0935  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.331   |\n",
      "|    critic2_loss    | 0.0134   |\n",
      "|    critic_loss     | 0.0062   |\n",
      "|    learning_rate   | 0.000522 |\n",
      "|    mean_cost_loss  | 0.365    |\n",
      "|    n_updates       | 1290     |\n",
      "|    std_cost_loss   | 0.0342   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0619  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.077   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.02    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.322   |\n",
      "|    critic2_loss    | 0.015    |\n",
      "|    critic_loss     | 0.00716  |\n",
      "|    learning_rate   | 0.000516 |\n",
      "|    mean_cost_loss  | 0.356    |\n",
      "|    n_updates       | 1320     |\n",
      "|    std_cost_loss   | 0.034    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0573  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.09 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.094   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.173   |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total timesteps | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0813  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.322   |\n",
      "|    critic2_loss    | 0.0123   |\n",
      "|    critic_loss     | 0.00572  |\n",
      "|    learning_rate   | 0.000512 |\n",
      "|    mean_cost_loss  | 0.356    |\n",
      "|    n_updates       | 1350     |\n",
      "|    std_cost_loss   | 0.0339   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560, episode_reward=-0.14 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.14    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.108   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.31    |\n",
      "|    critic2_loss    | 0.0122   |\n",
      "|    critic_loss     | 0.00564  |\n",
      "|    learning_rate   | 0.000508 |\n",
      "|    mean_cost_loss  | 0.343    |\n",
      "|    n_updates       | 1380     |\n",
      "|    std_cost_loss   | 0.0337   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0402  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0695  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0542  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.305   |\n",
      "|    critic2_loss    | 0.0124   |\n",
      "|    critic_loss     | 0.0059   |\n",
      "|    learning_rate   | 0.000505 |\n",
      "|    mean_cost_loss  | 0.339    |\n",
      "|    n_updates       | 1410     |\n",
      "|    std_cost_loss   | 0.0336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0455  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.149   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0438  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.306   |\n",
      "|    critic2_loss    | 0.0113   |\n",
      "|    critic_loss     | 0.00531  |\n",
      "|    learning_rate   | 0.000502 |\n",
      "|    mean_cost_loss  | 0.34     |\n",
      "|    n_updates       | 1440     |\n",
      "|    std_cost_loss   | 0.0335   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4830, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.149   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0531  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.159   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.302   |\n",
      "|    critic2_loss    | 0.0132   |\n",
      "|    critic_loss     | 0.00613  |\n",
      "|    learning_rate   | 0.000501 |\n",
      "|    mean_cost_loss  | 0.336    |\n",
      "|    n_updates       | 1470     |\n",
      "|    std_cost_loss   | 0.0334   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0594  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4950, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0412  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.024   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.285   |\n",
      "|    critic2_loss    | 0.0111   |\n",
      "|    critic_loss     | 0.00551  |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    mean_cost_loss  | 0.318    |\n",
      "|    n_updates       | 1500     |\n",
      "|    std_cost_loss   | 0.0331   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5010, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.106   |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5040, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -0.0182  |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5040     |\n",
      "---------------------------------\n",
      "[Training End]  steps: -2160\ttimes: 81.35296559333801\n"
     ]
    }
   ],
   "source": [
    "model = model.learn(**learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BSMarket instance> will be save as name. env_kwargs not in kwargs!\n",
      "<BSMarketEval instance> will be save as name. eval_env_kwargs not in kwargs!\n",
      "<Algorithms.ddpg.callbacks.ReportCallbacks object at 0x000001ED58CCF730> will be save as name. callback_kwargs not in kwargs!\n",
      "../logs/tb_logs/ddpg_220616-0010_1/config.yaml was saved.\n"
     ]
    }
   ],
   "source": [
    "config.save_config(f'{learn_kwargs[\"eval_log_path\"]}/config.yaml', env_kwargs, model_kwargs, learn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. P&L Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logs/tb_logs/ddpg_220615-2316_1/best_model\n"
     ]
    }
   ],
   "source": [
    "# model = model.load('../logs/tb_logs/ddpg_220607-2214_stable'+'/best_model')\n",
    "# model = model.load('../logs/tb_logs/ddpg_220607-2124_ntb_delta'+'/best_model')\n",
    "# model = model.load('../logs/tb_logs/ddpg_220608-2037_1'+'/best_model')\n",
    "# model = model.load('../logs/tb_logs/ddpg_220614-2216_1_ntb_pnl'+'/best_model')\n",
    "model = model.load('../logs/tb_logs/ddpg_220615-2254_1'+'/best_model')\n",
    "# model = model.load(learn_kwargs['eval_log_path'] + '/best_model')\n",
    "print(learn_kwargs['eval_log_path'] + '/best_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from Env.env import BSMarketEval\n",
    "\n",
    "eval_env_kwargs = env_kwargs.copy()\n",
    "\n",
    "eval_env_kwargs.update({\n",
    "    'n_assets': 1000,\n",
    "    'reward_mode': 'pnl',\n",
    "    'reward_fn': 'raw'\n",
    "})\n",
    "\n",
    "eval_env = BSMarketEval(**eval_env_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# random_pnl = np.mean([eval_env.pnl_eval() for _ in range(30)], axis=0)\n",
    "# delta_pnl = np.mean([eval_env.delta_eval() for _ in range(30)], axis=0)\n",
    "# rl_pnl = np.mean([eval_env.pnl_eval(model) for _ in range(30)], axis=0)\n",
    "\n",
    "# random_pnl = np.load('best_results/random_pnl.npy')\n",
    "# delta_pnl = np.load('best_results/ntb_pnl.npy')\n",
    "rl_pnl = np.load('best_results/rl_pnl_0614.npy')\n",
    "# rl_pnl = np.load('best_results/rl_pnl_eval_cash.npy')\n",
    "# ntb_pnl = np.load('best_results/ntb_pnl_eval_cash.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# np.save('best_results/rl_pnl_0614', ntb_pnl)\n",
    "# np.save('best_results/delta_pnl', delta_pnl)\n",
    "# np.save('best_results/rl_pnl', rl_pnl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# from Env.rewards import raw_reward\n",
    "# eval_env.reward_fn = raw_reward\n",
    "\n",
    "def pnl_eval(env, model):\n",
    "    obs = env.reset()\n",
    "    reward, done, info = 0, False, {}\n",
    "    total_raw_reward = 0\n",
    "    while not done:\n",
    "        if model:\n",
    "            action, _ = model.predict(obs, deterministic=False)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        if len(action.shape) > 1:\n",
    "            action = action.flatten()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_raw_reward += reward\n",
    "\n",
    "    return total_raw_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(1000,)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl_eval(eval_env, model).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# ntb_pnl = np.mean([eval_env.pnl_eval(model) for _ in range(30)], axis=0)\n",
    "ntb_pnl = np.mean([pnl_eval(eval_env, model) for _ in range(30)], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pnl_reward(pnl):\n",
    "    mean = np.mean(pnl)\n",
    "    std = np.std(pnl)\n",
    "    return mean - 0.02 * std , (mean, std)\n",
    "\n",
    "def sharpe_ratio(pnl):\n",
    "    return pnl.mean()/pnl.std()\n",
    "\n",
    "def var(pnl, ratio):\n",
    "    losses = np.sort(-pnl)\n",
    "    boundary = int(np.ceil(losses.shape[-1]*ratio))\n",
    "    return losses[boundary]\n",
    "\n",
    "def cvar(pnl, ratio=0.95):\n",
    "    losses = np.sort(-pnl)\n",
    "    boundary = int(np.ceil(losses.shape[-1]*ratio))\n",
    "    return np.mean(losses[boundary:], axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzQAAAG4CAYAAACTn6L9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABThElEQVR4nO3de3xU5b3v8e+aSyYTkplwSZCARUEFoSkgHLRURahXVKqW1nOsWtTW6tFyPKJWcdfSeqGVuqtuFaWKdtdbt1ARfe1aT7WnHt3eNt1gBGIFKxdTIEiSGZLMTGZmnT/CDBkyk2QyK5lZyef9es2LmWc967d+a9YzK/Nj1sxjmKZpCgAAAABsyJHvBAAAAACgtyhoAAAAANgWBQ0AAAAA26KgAQAAAGBbFDQAAAAAbIuCBgAAAIBtufKdgCTF43G1trZKklwulwzDyHNGAAAAAPLBNE1Fo1FJktfrlcPR9WcwBVHQtLa2qra2Nt9pAAAAACggEydO1JAhQ7rswyVnAAAAAGyrID6hcbkOpTFx4kS53e48ZoNsxGIxbd68WZMmTZLT6cx3OsgTxgEYA5AYB2AMoF2u46CtrS159VbHOiGTgihoOn5nxu12q6ioKI/ZIBuxWEySVFRUxIlrEGMcgDEAiXEAxgDaWTkOevLdei45AwAAAGBbFDQAAAAAbIuCBgAAAIBtFcR3aAAAAACp/fsXbW1t+U4DOUh8hyYUCmX8Do3b7bbse1YUNAAAAMg70zS1e/duNTY25jsV5Mg0TblcLm3fvr3LL/WXl5friCOO6NEX/7tCQQMAAIC8SxQzlZWVKikpyflNLvLHNE21trbK6/WmPY6maaqlpUV79+6VJI0aNSqn7VHQAAAAIK9isViymBk+fHi+00GOTNNUPB5XcXFxxsLU6/VKkvbu3avKysqcLj/jRwEAAACQV4nvzJSUlOQ5E/SnxPHO9TtTFDQAAAAoCFxmNrhYdbwpaAAAAADYFgUNAAAAANviRwEAAABQsJpaIwqGov2yrbJil/zeoqzWmTt3rj7//HNJ7ZdQeb1eTZgwQdddd51OOeUUSdJll12m999/P7lOcXGxxo0bpyuuuELz589Ptt9666168cUXk48dDoeGDRumc845RzfccINKS0uTy7Zs2aLHHntM//mf/6nGxkZVVVVp3rx5uuaaa1RcXNyj3G+99Va9/vrrevXVVzv9GMOECRP0r//6r5Kkyy+/PGOMCy+8UNdff72+/vWvd8r7ggsu0P/+3/9bLlfflhwUNAAAAChYwVBU6zbUKdDHRY2v2KX5U6uyLmgkacmSJZo3b57i8biampq0du1a/eAHP9Djjz+uWbNmSZKuvPJKXXnllTJNU8FgUK+//rpuu+02RaNRXXTRRclY55xzjm6//XZJUjwe1/bt27V48WI1Nzdr2bJlkqS3335b1157rc4880ytWLFCw4cP15YtW3TfffeptrZWjz76aI9zDwQC+sUvfqF777037fJp06bprbfeSj4++eST9S//8i+aNm2apPbirKmpSZL0wgsvaNSoUYpGo6qtrdXSpUvl9/t19dVXZ/FsZo+CBgAAAAUtEIqqqTW3X8LqS2VlZaqoqJAkjRw5Urfccovq6+u1bNkyvfzyy5Laf9Er0aeyslLjx49XS0uLli9frnPPPVcej0dSe4GQ6JeId9lll2nlypVatmyZIpGIbr/9dl144YX66U9/muxXVVWlCRMm6KyzztJHH32kL3/5y8llu3bt0te//nV9/PHHnXIfPXq0XnrpJS1YsEAzZ87stLyoqCglH0ny+/0pbYmCZtiwYaqoqJBpmvL5fLrkkkv0hz/8oc8LGr5DAwAAAFjs4osv1t/+9jdt3769yz779+/X+vXru4zldDrldrslSW+99Zb27NmjRYsWdeo3ZswYvfrqqynFTHdmzpypM844Qz/96U9z/vnkwyXmmulrFDQAAKB/tDZKjTtSb62NPe/bVX+gwIwfP16StHXr1ox9Ro0apZKSkox94vG4Nm/erGeeeSb5HZWNGzfqqKOOyjgB6ZFHHpl1rrfffrvq6ur05JNPZr1uJrt379bq1atTviPUV7jkDAAA9I9wQKpZ3f6vJHl8UvUCyVvefd/u+gMFpqysTJLU3Nzcbb+OfV5++WX98Y9/lNQ+4WQ8Htdpp52mm2++WZLU0NAgv9+fEuPWW29NriNJP/jBD3TNNdfo3HPPVV1dnUzTlKTk917OP/98/exnP0v2HzVqlK677jo99NBDOu+881RVVdWrfT7vvPNkGIbi8bhCoZDGjh2rb3zjG72KlQ0KGgAA0H/CAam1wfq+QIE5cOCAJKX8Mlk6zc3NKX3mzp2rm266SZLkcrk0fPjwlF8t8/l8CgaDKTFuuukmXXvttcn7iUvHVq5cqWg0qj179uiyyy7T2rVrM+a0cOFCvfTSS7rrrrv0yCOPZLm3Sm5v5MiRisVi2rVrl5588kldcsklWrdunYqKsv+xhZ6ioAEAAAAslvgC/rHHHpuxz65du3TgwIGUPkOGDNHYsWMzrjNlyhStWrVKjY2NKi8vlySNGDFCI0aMkKSU4mf06NGS2r+DI6nLuC6XSz/5yU906aWX6s9//nM3e5deVVWVxowZI9M0NXLkSE2YMEGnnnqq3n77bc2ZM6dXMXuC79AAAAAAFluzZo0mT57c5Xda1qxZo4qKCs2YMaPHcU899VRVVlam/WnmSCSihobef6o5Y8YMXXjhhbrzzjt7HaOjxKVusVjMkniZZP0Jzfbt2/Wzn/1Mf/3rX+X3+3XppZfqe9/7niTprrvu0m9/+9uU/j/+8Y916aWXWpMtAAAABh1fcd9fVJTLNoLBoOrr62WaphoaGrR69Wr9+7//u1atWpXs09LSovr6ekntc7+8+uqr+vWvf6277747q4knPR6P7r33Xl1zzTVqamrSt7/9bVVUVGjLli165JFHtGPHDk2ePDllnTFjxqT9yeZ0br75Zp1zzjk9zqej/fv3y+PxyDRN7d69W4899piGDh2qk046qVfxeiqrIxePx3X11VerurpaL774orZv364bb7xRI0eO1Pnnn69t27Zp8eLFuvDCC5PrdHfdIAAAAJBJ2cEJL/trW71xzz336J577pFhGBo2bJgmTZqkp556KuWTl1WrViULnPLych177LF68MEHNXfu3Ky3N3PmTK1Zs0YrV67UDTfcoC+++EKVlZU65ZRT9MADD+hLX/pSr/ZDap9L5sYbb9Qdd9yR9brf+ta3kvdLS0s1ffp0rVq1qs/rgayO2r59+3T88cdr6dKlKi0t1VFHHaWvfvWrWr9+fbKgueqqqzpNvgMAAAD0ht9bJL+3775Qnqs33nij2z6HX8GUyc9//vMeb/foo4/WsmXLetw/m+1dfPHFuvjii9MuS/dJz+GfAJmmqZaWFpWUlMgwjJxy7ImsvkNTWVmp+++/X6WlpTJNU+vXr9cHH3ygmTNn6sCBA9qzZ4+OOuqoPkoVAAAAAFL1+mLBuXPnqq6uTnPmzNFZZ52ljz76SIZh6NFHH9Wbb76p8vJyXXHFFSmXn/VELBbr8y8OwTqJY8UxG9wYB2AMQOp+HBimKSVuUvK+maZ/p77d9Edh6O25IBaLyTTN5A32ljiG3R3LxPE+/P1/tuOn1wXNgw8+qH379mnp0qVatmyZJk+eLMMwNG7cOF166aX64IMP9OMf/1ilpaU644wzehx38+bNvU0JeVRTU5PvFFAAGAdgDEBKPw7cbrfG+h2K7m9QvPkLSZJjiOQKBLV956bkvBmZ+nbVH4WnN+cCl8ul1tZWxePxPsgI+dDa2trl8nA4rLa2NtXW1ua0nV4XNNXV1clEbrrpJv31r3/VnDlzkr+HPXHiRH322Wd67rnnsipoJk2a1KcT78BasVhMNTU1qq6uTv7GOQYfxgEYA5C6HwdGYJc0bKjkPdjgHSr5yjR5zJju+3bTH4Wht+eCUCik7du3y+v1psyjAnsyTVOtra3yer1dfofG4XDI7XbrmGOOSTnukUgkqw85sv5RgA0bNuj0009Pth1zzDFqa2vTgQMHNGzYsJT+48aN07vvvpvNJuR0OvljaEMcN0iMAzAG0C7jODCMQ7eOj3vSt7v+KCjZngucTqcMw0jeMDB0dzwTyw8fL9n+HcnqRwF27dql66+/Xnv27Em2ffTRRxo2bJh++9vfauHChSn9a2trNW7cuKwSAgAAAICeyqqgqa6u1uTJk7VkyRJt3bpVf/nLX7R8+XJdc801mjNnjj744AM98cQT2rFjh5599lmtXbtWV155ZV/lDgAAAGCQy+qSM6fTqUceeUR33nmnLr74Ynm9Xl122WW6/PLLZRiGHnjgAT344IN64IEHNHr0aN13332aNm1aX+UOAAAAYJDL+kcBRo4cqYceeijtstNPPz3l+zUAAADAQDZ37lx9/vnnktq/E+L1ejVhwgRdd911OuWUUyRJl112md5///3kOsXFxRo3bpyuuOIKzZ8/P9l+66236sUXX0w+djgcGjZsmM455xzdcMMNKi0tTS7bsmWLHnvsMf3nf/6nGhsbVVVVpXnz5umaa64ZdD+s0OtfOQMAAAD6XGujFA70z7Y8PslbnvVqS5Ys0bx58xSPx9XU1KS1a9fqBz/4gR5//HHNmjVLknTllVfqyiuvlGmaCgaDev3113XbbbcpGo3qoosuSsY655xzdPvtt0uS4vG4tm/frsWLF6u5uVnLli2TJL399tu69tprdeaZZ2rFihUaPny4tmzZovvuu0+1tbV69NFHc38ubISCBgAAAIUrHJBqVvd9UePxSdULelXQlJWVqaKiQlL71Uy33HKL6uvrtWzZMr388suSpJKSkmSfyspKjR8/Xi0tLVq+fLnOPfdceTweSe2f3iT6JeJddtllWrlypZYtW6ZIJKLbb79dF154oX76058m+1VVVWnChAnJCe+//OUv9/aZsB0KGgAAABS2cEBqbch3Flm5+OKL9Z3vfEfbt2/vss+KFSu0fv365Cc56TidTrndbknSW2+9pT179mjRokWd+o0ZM0avvvqqjjzyyNx3wEay+pUzAAAAAN0bP368JGnr1q0Z+4waNUolJSUZ+8TjcW3evFnPPPOMvv71r0uSNm7cqKOOOkrDhw9Pu85gK2YkPqEBAAAALFdWViZJam5u7rZfxz4vv/yy/vjHP0qS2traFI/Hddppp+nmm2+WJDU0NMjv96fEuPXWW5PrSNIPfvADXXPNNZbshx1Q0AAAAAAWO3DggCSl/DJZOs3NzSl95s6dq5tuukmS5HK5NHz48JRfLfP5fAoGgykxbrrpJl177bXJ+21tbZbsg11Q0AAAAAAW+/jjjyVJxx57bMY+u3bt0oEDB1L6DBkyRGPHjs24zpQpU7Rq1So1NjaqvLxckjRixAiNGDFCkgbdTzZLfIcGAAAAsNyaNWs0efLkLr/TsmbNGlVUVGjGjBk9jnvqqaeqsrIy7U8zRyIRNTTY68cTrMAnNAAAAEAOgsGg6uvrZZqmGhoatHr1av37v/+7Vq1alezT0tKi+vp6SVIgENCrr76qX//617r77rvlcvX8LbnH49G9996ra665Rk1NTfr2t7+tiooKbdmyRY888oh27NihyZMnW76PhYyCBgAAAIXN4yvobdxzzz265557ZBiGhg0bpkmTJumpp55K+eRl1apVyQKnvLxcxx57rB588EHNnTs36+3NnDlTa9as0cqVK3XDDTfoiy++UGVlpU455RQ98MAD+tKXvtTrfbEjChoAAAAUrsSEl/21rSy98cYb3fb57W9/26NYP//5z3u83aOPPlrLli3rcf+BjIIGAAAAhctb3n4DMuBHAQAAgH0YvHUBkIpPaAAAQPdaG6VwILXN4+vf/zl3eSUZUuOOzsv6OxcABYOCBgAAdC8ckGpWHypqEt9r6M8iwlkkRZql2ldSi6t85AKgYFDQAACAngkHpNYCmOOiUPKA5UzTzHcK6EdWHW8uRAUAAEBeud1uSe1ztWDwSBzvxPHvLT6hAQAAQF45nU6Vl5dr7969kqSSkhIZhpHnrNBbpmkqHA7L4XCkPY6maaqlpUV79+5VeXm5nE5nTtujoAEAAEDeHXHEEZKULGpgX6Zpqq2tTW63u8vCtLy8PHncc0FBAwAAgLwzDEOjRo1SZWWl2tra8p0OchCLxVRbW6tjjjkm46cvbrc7509mEihoAAAAUDCcTqdlb3SRH7FYTJJUXFzcL8eSHwUAAAAAYFsUNAAAoHeMzm8jcv21IgDIFpecAQAwULQ2pk44KbVPOtkXE066vJIMqXFHsskwTY31O2SEA1LJUOu3CQBpUNAAADBQhANSzepDRY3HJ1Uv6JuCxlkkRZql2lcObc80FQ+Z0slXUdAA6DcUNAAADCThgNTakJ/tmabirf23aQCQ+A4NAAAAABujoAEAAABgWxQ0AAAAAGyLggYAAACAbVHQAAAAALAtChoAAAAAtkVBAwAAAMC2mIcG6CdNrREFQ1FJUlmxS35vkaX9AQAABiM+oQH6STAU1boNdVq3oS5ZqFjZHwAAYDDiExqgHwWyLEyy7Q8AADDY8AkNAAAAANuioAEAAABgW1kXNNu3b9dVV12ladOm6bTTTtPjjz+eXLZz504tXLhQU6dO1bx58/TWW29ZmiwAAAAAdJRVQROPx3X11Vdr6NChevHFF/XTn/5UK1as0MsvvyzTNHXddddpxIgRWrNmjb7xjW/o+uuvV11dXV/lDgAAAGCQy+pHAfbt26fjjz9eS5cuVWlpqY466ih99atf1fr16zVixAjt3LlTzz//vEpKSjR+/Hi98847WrNmjX74wx/2Vf4AAAAABrGsPqGprKzU/fffr9LSUpmmqfXr1+uDDz7QzJkztXHjRk2aNEklJSXJ/tOnT9eGDRuszhkAAAAAJOXws81z585VXV2d5syZo7POOkv33HOPKisrU/oMHz5cu3fvzipuLBZTLBbrbVroZ4ljxTHrnmmaMs148n53z1m2/fOJcQDGQGEwTFNK3KTkfdOC49IptkwZMmV2aDPjpiRDppl+LGSTX+ftpd+m1fuJ3HAugJT7OMh2vV4XNA8++KD27dunpUuXatmyZWptbVVRUepM5kVFRYpEIlnF3bx5c29TQh7V1NTkO4WC5na75fJXqrGhQZIUDA7Tpl3b1NbWZkn/QsE4AGMgf9xut8b6HYrub1C8+QtJkmOI5AoEtX3nppzOH+liu4yhKo3GdKCxSdHgF8m+jiHDdSAY1PZdqdvMJr90fbveZs/30+l06sgKn1zRlk7Loq4S7awPdPtmyooYAx3nAkj9Nw56XdBUV1dLksLhsG666SZ985vfVGtra0qfSCSi4uLirOJOmjSpU2GEwhWLxVRTU6Pq6mo5nc58p1PQ6ppCKh86VJJUVlamqjEVlvbPJ8YBGAOFwQjskoYNlbwHG7xDJV+ZJo8ZY33scr8Ml1P+cr/kOfhpctxUY1gqLUu/zWzy69Q3wzZ7s59GYJdU80cpHDjU6PFJ1QtUfvD9TX/EGIg4F0DKfRxEIpGsPuTI+kcBNmzYoNNPPz3Zdswxx6itrU0VFRX69NNPO/U//DK07jidTl4ANsRx655hGDIMR/J+d89Xtv0LAeMAjIE8M4xDt46PrTgmh8eWIcmQ0aEt7jjU1ZFum9nk12l76bfZbZxM+xIJSqHG/MYYwDgXQOr9OMh2nax+FGDXrl26/vrrtWfPnmTbRx99pGHDhmn69OnatGmTQqFQctn69es1ZcqUrBICAAAAgJ7KqqCprq7W5MmTtWTJEm3dulV/+ctftHz5cl1zzTWaOXOmRo0apdtuu02ffPKJVq5cqQ8//FALFizoq9wBAAAADHJZFTROp1OPPPKIvF6vLr74Yt1+++267LLLdPnllyeX1dfX66KLLtK6dev08MMPq6qqqq9yBwAAADDIZf2jACNHjtRDDz2UdtnYsWP19NNP55wUAAAAAPREVp/QAAAAAEAhoaABAAAAYFsUNAAAAABsq9cTawIDSVNrRMFQVGXFLvm9A39i18T+Sho0+wwAAAYmPqEBJAVDUb1Ruzf5Jn+gC4aiWrehTus21A2afQYAAAMTn9AAB7VEYvlOoV8FKGQAAMAAwCc0AAAAAGyLggYAAACAbVHQAAAAALAtChoAAAAAtkVBAwAAAMC2KGgAAAAA2BYFDQAAAADbYh4aoIA0tUYUDEXlNAxFY2a+0wEAACh4FDRAAQmGolq3oU6lHpdmHTM83+kAAAAUPAoaoMAEQtF8pwAAAGAbfIcGAAAAgG1R0AAAAACwLQoaAAAAALZFQQMAAADAtihoAAAAANgWBQ0AAAAA26KgAQAAAGBbzEMD2ExTa0TBUFRlxS75vUX5TgcAACCv+IQGsJlgKKo3avcqyAScAAAAFDSAHbVEYvlOAQAAoCBQ0AAAAACwLQoaAAAAALZFQQMAAADAtihoAAAAANgWBQ0AAAAA26KgAQAAAGBbFDTAIGcY+c4AwKBmWPRWxKo4AGzHle8EgIGiqTWSnOyyrNglv7cozxl1r9jtkMMwtKuhxTY5AxhAXF5JhtS4I7XdcEqxLCYPzhTH45O85TkmCaDQUdAAFgmGolq3oU6SNH9qlS2KgyKnQ83hqP5j2xeaO7HSFjkDGECcRVKkWap9RQoHDrX7RktHz84tjscnVS+goAEGAQoawEKBUBb/o1hAWiKxfKcAYDALB6TWhkOPPX5r4gAYFLjgFAAAAIBtUdAAAAAAsK2sC5o9e/Zo0aJFmjlzpk455RQtW7ZM4XBYknTXXXdpwoQJKbenn37a8qQBAAAAQMryOzSmaWrRokXy+Xx65pln1NTUpCVLlsjhcOhHP/qRtm3bpsWLF+vCCy9MrlNaWmp50gAAAAAgZfkJzaeffqoNGzZo2bJlOvbYYzVjxgwtWrRIr7zyiiRp27ZtmjRpkioqKpI3r9fbJ4kDAAAAQFYFTUVFhR5//HGNGDEipf3AgQM6cOCA9uzZo6OOOsrK/AAAAAAgo6wuOfP5fDrllFOSj+PxuJ5++mmddNJJ2rZtmwzD0KOPPqo333xT5eXluuKKK1IuP+uJWCymWIyfkLWLxLGy+zEzTTN56+2+tK8fT94/PE53yzv2OdSvc38rc+0YP5d4A2UcoPcYA4XBME0pcZMO/mvItOC4dIotU4bazx2JNjN+cHtypB0LPYlhWXsX+945j6779+z5UPKxFc+3XXEugJT7OMh2vZzmoVm+fLk2b96s1atXa9OmTTIMQ+PGjdOll16qDz74QD/+8Y9VWlqqM844o8cxN2/enEtKyJOampp8p9BrbrdbLn+lAk1NCgZLtWnXNrW1tfUqRmND+/wHweCwlDjdLe/Up6RIsWiFmpqaZMbjyf6SrM314HaCwWCv43Vk53EAazAGrON0OnVkhU+uaEunZVFXiXbWB1L+6Lvdbo31OxTd36B48xftMYaWqDQaU6iu9mCx0XWMTNLFdhlDVRqN6UBjk6LBLw7lPfRIxWKdt2k43Sp2SQcaGxULdh3DivZM+54uj6769/S5liTHEMkVCGr7zk05nUsHAs4FkPpvHPS6oFm+fLl+85vf6Fe/+pWOO+44HXvssZozZ47Ky8slSRMnTtRnn32m5557LquCZtKkSSoqYrZyu4jFYqqpqVF1dbWcTme+0+m1uqaQfH6/ysrKVDWmotcxyocOlaS0cbpb3rGPr9glp8slv9/fqb+VuSa2U1ZWllO8gTIO0HuMgb5hBHZJNX9snzAyweOTqheovLo6ff9hQ6XE11eHVsoww3L9/dUex+gyl46xy/0yXE75y/2S5+CnvXFTAadHLjOs0sO36RstY9zs9vcJHjNjDMvaM+17ujwy9c/muZYk71DJV6bJY8b05CkdkDgXQMp9HEQikaw+5OhVQXPnnXfqueee0/Lly3XWWWdJkgzDSBYzCePGjdO7776bVWyn08kLwIbsftwMw0jeersf7es7kvcPj9Pd8o59DvXr3N/KXDvGzyVegt3HAXLHGLCYYUiRoBRqTG0zDCnd85xYZhiJBkmGjGxidJVLutgd2uKODksO32Zxeaf+6WJY155h39PmkaF/Vs91N/0HGc4FkHo/DrJdJ+t5aB566CE9//zz+ud//mede+65yfYHHnhACxcuTOlbW1urcePGZbsJAAAAAOiRrAqabdu26ZFHHtH3v/99TZ8+XfX19cnbnDlz9MEHH+iJJ57Qjh079Oyzz2rt2rW68sor+yp3AAAAAINcVpecvf7664rFYlqxYoVWrFiRsuzjjz/WAw88oAcffFAPPPCARo8erfvuu0/Tpk2zNGEAAAAASMiqoLn66qt19dVXZ1x++umn6/TTT885KQAAAADoiay/QwMAAAAAhYKCBgAAAIBtUdAANpDyy6IAYAWDtwAABoZeT6wJoHtNrREFQ1E5DUPRmNn9CmkUux1yGIZ2NbTkFAdAgWttTJ0A0nBKsWjfbMvllWRIjTs6L/P4JG9532wXAPoABQ3Qh4KhqNZtqFOpx6VZxwzvVYwip0PN4ahe27QnpzgAClw4INWsPlTU+EZLR8/um205i6RIs1T7SmoR5fFJ1QsoaADYCgUN0McCIWv+h9WqOAAKWDggtTa03/f4+3d7AGBTXEALAAAAwLYoaAAAAADYFgUNAAAAANuioAEAAABgWxQ0AAAAAGyLggYAAACAbVHQAAAAALAt5qEBBomm1oiCoaichqFozMx3OoA9tTamTkQptU9GyUSUAJA3FDTAIBEMRbVuQ51KPS7NOmZ4vtMB7CkckGpWHypqPD6pegEFDQDkEQUNMIgEQtF8pwDYXzggtTbkOwsAwEF8hwYAAACAbVHQAAAAALAtChoAAAAAtkVBAwAAAMC2KGgAAAAA2BYFDQAAAADboqABACAXBn9KASCfmIcGAIDecnklGVLjjtR2wyE5i6RoqPM6Hh8TcaJrrY2HJm/tiLEDpEVBAwBAbzmLpEizVPtK6htQ32jp6Nmd2z0+qXoBb0rRtXBAqlnN2AF6iIIGAIBchQNSa8Ohxx5/+nagpxg7QI9x4S8AAAAA26KgAQAAAGBbFDQAAAAAbIuCBgAAAIBtUdAAAAAAsC0KGgAAAAC2RUEDAEB/MvjTayscL6DgMQ8N0ANNrREFQ1FJUlmxS35vUZ4zAmBLLq8kQ2rckdpuOKVYNC8poQtWHq/WxtSJMqX2yTKZKBPIGQUN0APBUFTrNtRJkuZPraKgAdA7ziIp0izVvpL65tY3Wjp6dv7yQnpWHq9wQKpZfSiOxydVL6CgASxAQQP0UCDE/54CsMjhs8B7/PnLBd2z6ngdHgeAJbgwFAAAAIBtUdAAAAAAsK2sCpo9e/Zo0aJFmjlzpk455RQtW7ZM4XBYkrRz504tXLhQU6dO1bx58/TWW2/1ScIAAAAAkNDjgsY0TS1atEitra165pln9Ktf/Up//vOfdf/998s0TV133XUaMWKE1qxZo2984xu6/vrrVVdX15e5AwAAABjkevyjAJ9++qk2bNigt99+WyNGjJAkLVq0SL/4xS906qmnaufOnXr++edVUlKi8ePH65133tGaNWv0wx/+sM+SBwAAADC49fgTmoqKCj3++OPJYibhwIED2rhxoyZNmqSSkpJk+/Tp07VhwwbLEgUAAACAw/X4Exqfz6dTTjkl+Tgej+vpp5/WSSedpPr6elVWVqb0Hz58uHbv3p11QrFYTLFYLOv1kB+JY2X3Y2aaZvKWbl/al8WT93vaJ9F2qL1nMTr2T38/c65d76N18TqKxWJyu922HwfovYFyLuiOYZpS4iZJMmWo/fVzqM2q9gx9Dz420zzXOeeXY2wzbkpOyZRy28e+fP76MkY2z59pSjJ6+Fx33b+QDJZzAbqW6zjIdr1ez0OzfPlybd68WatXr9ZTTz2loqLUiQaLiooUiUSyjrt58+bepoQ8qqmpyXcKveZ2u+XyVyrQ1KRgsFSbdm1TPB6Xf8QRao1JLqdDhtOtpsZGmaapYHCYNu3apra2tk4xGhva5xcIBofpb3t2yCwZ2t5WUqRYtEJNTU0y4/FkjEzbkded7K+wq9P9YDCYzLVjHt3t4+G5dBfP6XQm85Mkr1Nq2rc75UST6OPyV2rrP/an7YPBw87ngu643W6N9TsU3d+gePMXkiSXMVSl0ZgONDYpGvwi2deK9kx9HUMkVyCo7Ts3dToP5ZqfJbErhimW4z725fPXlzGyef6cQ0tUGo0pVFfbXggeZDjdKnZJBxobFesQO1P/qKtEO+sDBXfOHcjnAvRcf42DXhU0y5cv129+8xv96le/0nHHHSePx6PGxsaUPpFIRMXFxVnHnjRpUqfiCIUrFouppqZG1dXVcjqd+U6n1+qaQvL5/SorK1PVmIpk25831KnU49LXjh0hf3m5JKX0OTxG+dChKX0Sbb5il5wul/x+f6cY6bbTsX+6+2VlZRnz6GofD8+lJ/ES+UnS/KlVqq5O06cxpLVvbpHP79M3po5O2wcD20A5F3THCOyShg2VvAcbyv0yXE75y/2SJ36ooxXtmfp6h0q+Mk0eM8b6/HKMbcZNBSQ5c93Hvnz++jJGNs/f0EoZZliuv7/aPuFmgm+0jHGzVV5eLnk6fEKTrr/HJ1UvUHl1daft5ctgORega7mOg0gkktWHHFkXNHfeeaeee+45LV++XGeddZYkaeTIkdq6dWtKv3379nW6DK0nnE4nLwAbsvtxMwwjeUvsh2EYCoZjMgzj4GNHsj3dvravn9on0XaovXOMdNvp2D/9fSNjHl3vY/bxEvl1te8ypKaWsAxPNOu8MLDY/VzQLcM4dGtvkNT++jnUZlV7hr6Jx2lfiznml2PsuKPDklz2sS+fv76M0ZvnLxKUQo2H+hWXd73Njv272l6eDfhzAXqkt+Mg23WymofmoYce0vPPP69//ud/1rnnnptsnzJlijZt2qRQKJRsW79+vaZMmZJVMgAAAACQjR4XNNu2bdMjjzyi73//+5o+fbrq6+uTt5kzZ2rUqFG67bbb9Mknn2jlypX68MMPtWDBgr7MHQAAAMAg1+NLzl5//XXFYjGtWLFCK1asSFn28ccf65FHHtHtt9+uiy66SGPHjtXDDz+sqqoqyxMGAAAAgIQeFzRXX321rr766ozLx44dq6efftqSpAAAAACgJ7L6Dg0AAAAAFBIKGgAAAAC2RUEDQNJhvw4KAOgdg7dWQH/r1cSaQL41tUYUDEVVVuyS38tErInnQ1KvnhOPyyGHYWhXQ0uvYwDAoOfySjKkxh2p7YZTikXzkhIwGFDQwJaCoajeqN2ruRMreeOt9udj3YY6SdL8qVVZPydup0PN4ahe27Sn1zEAYNBzFkmRZqn2FSkcONTuGy0dPTt/eQEDHAUNbKslEst3CgUlEMr9f/+siAEAg144ILU2HHrs8ecvF2AQ4EJPAAAAALZFQQMAAADAtihoAAAAANgWBQ0AAAAA26KgAQAAAGBbFDQAAAAAbIuCBrApw8h3BgAAAPnHPDSADXlcDjkMQ7saWuQ0DEVjZr5TAgAAyAsKGsCG3E6HmsNRvbZpj0o9Ls06Zni+UwIAAMgLChrAxgKhaL5TAAAAyCu+QwMAAADAtihoAAAAANgWBQ0AAAAA26KgAQAAAGBbFDQAAAAAbIuCBgAAAIBtUdDA1gyjMLeZj7wKxWDed2BAMHhrAMBemIcGtuVxOeQwDO1qaJEklRW75PcW9ek2i93dbzPR5x+NrYrGzD7NJ5+aWiMKhqJyGoZicckwjJTnpz+OB5BWa6MUDnRu9/gkb3l/Z2MvLq8kQ2rckdpuOKUY817ZCq8DDCIUNLAtt9Oh5nBUr23aI0maP7Wqz99AF/Vgm4k+G3c2asqR5X2aTz4FQ1Gt21CnUo9Ls44ZLhlGct//Y9sXmjuxkoIG+REOSDWrU9/MeXxS9QLeyHXHWSRFmqXaV1KfP99o6ejZ+csL2eN1gEGEgga2Fwj1//8a9mSbrW3xfsgkvzI9Dy2RWD9nAhwmHJBaG/KdhX0d/vx5/PnLBb3H6wCDBBfKAgAAALAtChoAAAAAtkVBAwAAAMC2KGgAAAAA2BYFDQAAAADboqABAAAAYFsUNEAHA2GW+4GwDwCAPmLw1g8DD/PQAAd5XIdmuXcahqIxM98pZa3YfWgfJKms2MXklgCAdi6vJENq3NF5mcfHhJuwLQoa4CD3wVnuX9u0R6Uel2YdMzzfKWWtqMM+SNL8qVUUNACAds4iKdIs1b7SPulmgscnVS+goIFtUdAAhwmEovlOIWcDYR8AAH0kHJBaG/KdBWAZLqQEAAAAYFsUNAAAAABsq9cFTSQS0Xnnnaf33nsv2XbXXXdpwoQJKbenn37akkQBAAAA4HC9+g5NOBzW4sWL9cknn6S0b9u2TYsXL9aFF16YbCstLc0tQwAAAADIIOtPaLZu3apvf/vb2rGj80/+bdu2TZMmTVJFRUXy5vV6LUkUAAAAAA6XdUHz/vvv68QTT9Tvfve7lPYDBw5oz549Ouqoo6zKDQAAAAC6lPUlZ5dcckna9m3btskwDD366KN68803VV5eriuuuCLl8rOeiMViisVi2aaFPEkcq/4+ZqZpyjTNg/fjkiRDZq/zODxeImZX9w/fZnuMjn3Sx+t+vZ7c7z7XRL+ebcfsdr0u807MQWqmPp+8lgePfJ0L0jFMU0rcEg4+NnPMr3NsU4YOnj86bs+S9gx9u9iXfOdnxk3J2X4qyGkf+/L56/cYfRg7q7HQRWyLXh9SYZ0LkD+5joNs17NsHppPP/1UhmFo3LhxuvTSS/XBBx/oxz/+sUpLS3XGGWf0OM7mzZutSgn9qKamJqv+TqdT/hFHqPXgePU6paZ9u1MGcKY+DodDLn+lgsGAYlG/mpqaVOxyKBaL6W+f75Npmsm+krrdjtvt7hRPYZdi0You73fcptNhyHC61dTYKHndikUrdKD5QNp43a3Xk+1nit3xvhmPKxgcpk27tkmSXP5KNTY0SCVFXcbLtJ63yJU571iFJCkQaFIsWqFgMKhgsFSbdm1TW1tb2mPqcBgq83oVaG6VKTPtsYH9ZHsusJrb7dZYv0PR/Q2KN3+RbHcMkVyBoLbv3JQyJnON7TKGqjQa04HGJkWDh7ZnRXumvpn2pVDyc1UMUyzXGH2ZXz/H6MvY2YyFrmJb8fo4XL7PBSgM/TUOLCtoLrjgAs2ZM0fl5eWSpIkTJ+qzzz7Tc889l1VBM2nSJBUVMbO5XcRiMdXU1Ki6ulpOpzOrdeuaQvrzhjpJ7TPaV1dX9LhPXVNIZWUhOV0u+f1++YpdisQNvbE1kLZvT7ZzeLye3E9ss9Tj0teOHSF/eXmyT+mQ0l6t15PtdxU7cV+SysrKVDXm0PNQPnRot/G6Wi9j3s72U4nP55fT5VJZWVlKjHTHtD1Gid7Y1vmYwX5yORdYzQjskoYNlTp+hdM7VPKVafKYMdbGLvfLcDnlL/dLnvihjla0Z+rbxb7kOz8zbiogyZnrPvbl89ffMfoydjZjoavYFr0+pMI6FyB/ch0HkUgkqw85LCtoDMNIFjMJ48aN07vvvptVHKfTyQvAhnpz3AzDUDAcS95Pt36mPoZhyDCMg/cdMoz2r4Nl6tuT7aSL15P7wXAsw7pdx8u8Xk/ud79e+ues+3jdrZc2byPxRKY+n10d00SM7o4N7KUgzuGGceh2eFuuuXWKbUg6eP7ouD1L2jP07Wpf8pxf3NFhSS772JfPX7/H6MPYWY2FLmJb9frooCDOBci73o6DbNexbGLNBx54QAsXLkxpq62t1bhx46zaBAAAAACksKygmTNnjj744AM98cQT2rFjh5599lmtXbtWV155pVWbAAAAAIAUlhU0X/nKV/TAAw/opZde0nnnnaff/va3uu+++zRt2jSrNgEAAAAAKXL6Ds3HH3+c8vj000/X6aefnlNCAAAAANBTln1CAwAAAAD9jYIGAAAAgG1R0GBQSfn1SgAA7M7grRxg2Tw0QKErdjvkMAztamiRJJUVu+T3MokrgDRaG6VwILXNcEqxaF7SAdJyeSUZUuOO1HbGKgYZChoMGkVOh5rDUb22aY+k9pnpKWgApBUOSDWrU4sa32jp6Nn5ywk4nLNIijRLta8wVjGoUdBg0AmE+F8rAD0QDkitDYcee/z5ywXoCmMVgxwXXgIAAACwLQoaAAAAALZFQQMAAADAtihoAAAAANgWBQ0AAAAA26KgAQAAAGBbFDTAAGYY/bue1TEAZMDs8ACQxDw0wABV7HbIYRja1dAip2EoGjP7dL2OPK5DMSSprNjFJKYoTK2NqRMSSoU/yzqzwwNACgoaYIAqcjrUHI7qtU17VOpxadYxw/t0vY7cHWJI0vypVRQ0KEzhgFSz2l6zrDM7PACkoKABBrhAqHf/Y9vb9ayOAfQ5u86ybte8AcBiXIQLAAAAwLYoaAAAAADYFgUNAAAAANuioAEAAABgWxQ0AAAAAGyLggYAAACAbVHQAAAAALAt5qFBwWlqjSgYivZ6lnoAAAAMHhQ0KDjBUFTrNtT1epZ6AAAADB4UNChIzDAPAACAnuA7NAAAAABsi4IGAAAAgG1R0AAAAACwLQoaAAAAALZFQQMAAADAtihoAAAAANgWBQ0GJMPIdwYACo7BnzwAGIiYhwYDTrHbIYdhaFdDi5yGoWjMzHdKAPLN5ZVkSI07DrUZTinGnFcAYHcUNBhwipwONYejem3THpV6XJp1zPB8pwQg35xFUqRZqn1FCgfa23yjpaNn5zcvAEDOKGgwYAVC/M8rgMOEA1JrQ/t9jz+/uQAALMEFxQAAAABsi4IGAAAAgG31uqCJRCI677zz9N577yXbdu7cqYULF2rq1KmaN2+e3nrrLUuSBAAAAIB0elXQhMNh3Xjjjfrkk0+SbaZp6rrrrtOIESO0Zs0afeMb39D111+vuro6y5IFAAAAgI6y/lGArVu3avHixTLN1J/Cfffdd7Vz5049//zzKikp0fjx4/XOO+9ozZo1+uEPf2hZwgAAAACQkPUnNO+//75OPPFE/e53v0tp37hxoyZNmqSSkpJk2/Tp07Vhw4ackwQAAACAdLL+hOaSSy5J215fX6/KysqUtuHDh2v37t1ZxY/FYorFYtmmhTxJHKveHDPTNGWa8eT9RIxE+6FlmfqYyeUd+/bkviQZsi5e+vtWx+vL2DnGS3xga6pTvPTHruvjC/vJ5VxgNcM0pcQtyZShg69z0+yiLdt2K2IMnPzMuCk5208FhZhfoT9/ecvv4GPTgtdvIZ0LkD+5joNs17NsHprW1lYVFRWltBUVFSkSiWQVZ/PmzValhH5UU1OTVX+32y2Xv1KNDe3zQQSDw7Rp1zZJOtReUqRYtEJNTU0y4/FOfYLBgGJRv5qamqSwK9m3J/eLXQ7FYjH97fN9cjoMGU63DhwI9jpeuvsHmg9YGq8vY+ccL1YhSQoEmjrFS3fsujq+8Xhc/hFHqDUmORyGyrxeBZpbZcqU1yk17dvNH8oClu25wGput1tj/Q5F9zco3vxFst1lDFVpNKYDjU2KBr/I2JZtuxUxBlx+FcMUK+T8Cv35y0N+jiGSKxDU9p2b1NbWJivk+1yAwtBf48Cygsbj8aixsTGlLRKJqLi4OKs4kyZN6lQYoXDFYjHV1NSourpaTqczq3XrmkIqHzpUklRWVqaqMRUp7b5il5wul/x+f9o+ZWWh5PKOfXt6PxI39MbWgEo9Ln3t2BEqLS3LKd7h90uHlFoary9j5xzP2X4q8fn8neL19vj+eUPdwWNToje2tc/sPn9qlaqrK3o9XtF3cjkXWM0I7JKGDZW8HRrL/TJcTvnL/ZInnrkt23YrYgyg/My4qYAkZ4HmV+jPX97y8w6VfGWaPGaMclVI5wLkT67jIBKJZPUhh2UFzciRI7V169aUtn379nW6DK07TqeTF4AN9ea4GYYhw3Ak7yfWT7QfWpapj5Fc3rFvNveD4ViHOLnHS71vdby+jJ1jPCNxUNUpXvpj1/3x7XhsguFYpz4oTAVxDjeMQ7dDjZIOnjcMo4u2bNutiDFw8os7OiwpwPwK/fnLW36Jxxa+dgviXIC86+04yHYdyybWnDJlijZt2qRQKJRsW79+vaZMmWLVJgAAAAAghWUFzcyZMzVq1Cjddttt+uSTT7Ry5Up9+OGHWrBggVWbAAAAAIAUlhU0TqdTjzzyiOrr63XRRRdp3bp1evjhh1VVVWXVJgAAAAAgRU7fofn4449THo8dO1ZPP/10TgkBAAAAQE9Z9gkNAAAAAPQ3ChoAAAAAtkVBA6DPpfxqaAHEQd/gJ1oBGzN4Swj7smweGqAnmlojCoaichqGojEz3+mgHxS7HXIYhnY1tOR03DvGKSt2ye/tegLexFiT1KP+SKO1UQoHOrd7fJK3vFPzkRW+9kktO1aeGfoCKCAuryRDatyR2s7rFzZBQYN+FQxFte7gDPCzjhme73TQD4qcDjWHo3pt056cjnsizn9s+0JzJ1Z2W6AkxpokzZ9aRUHTG+GAVLM6tajx+KTqBWnf5LiiLVLNH6VIsNu+AAqIs0iKNEu1rxx6vfP6hY1Q0KDfBQ7+rzkGF6uOe0sk1u/bHNTCAam1Ibv+ocY+SwdAH8r29Q4UCC6YBAAAAGBbFDQAAAAAbIuCBgAAAIBtUdAAAAAAsC0KGgAAAAC2RUEDAAAAwLYoaJAzZgeHFTrOxYgBhNnHAfvi9QubYB4a9EpiFnbTNOUfcUSndsOQPC6HQm1xZmlHt4rdDjkMQ7saWuQ0DEVjZr5TghUyzT5uONon8ouGOq/DzORAYcj0+pV4naLgUNCgVxKzsJumqTnjyzq1J2aE7+ms7hjcipwONYejem3TnuTYwQCQbvZxSfKNlo6e3bmdmcmBwpHp9cvrFAWIgga9FghFZZrxtO0J2czqDnQcOxhADp993ONP3w6g8PA6hQ1wcSQAAAAA26KgAQAAAGBbFDQAAAAAbIuCBgAAAIBtUdAAAAAAsC0KGgAAAAC2RUED22AmeWAQYGZywPbcbne+U8Agwzw0SGpqjSgYiqqs2FVwE2EykzwySYxbSQU5dpGFdDOTG472Cf6iodS+GdudUoz5jIC8aG2UEWrSWL9DRmBX+/9EenzpJ+FsbUydsDMhU3+gCxQ0SAqGonqjdq/mTqwsuDeFzCSPTIKhqNZtqJMkzZ9aVXBjF1lINzO5b7R09OzOs5V31w6g/4UDUs1qRf+xXRo2VCr2S9UL0hcoB/umvH49vsz9gS5Q0CBFSySW7xS6xEzySIdxMcB0nJnc4+/c1pN2APkRDije/IXkVffXih/++gV6iYuVAQAAANgWBQ0AAAAA26KgAQAAAGBbFDQAAAAAbIuCBgAAAIBtUdAAAAAAsC0KGgADXne/HIosGen/dBg80cDgkOEcAOQL89AAGNCK3Q45DEO7GlokSWXFrsyTb6abuZpZq1O5vJIMqXFHSrMhh4qL+JMCDHgZzgEynFKMOcGQH/z1ATCgFTkdag5H9dqmPZKk+VOrMhc0h89czazVnTmLpEizVPtKavFXViXjSyfnLy8A/SPTOcA3Wjp6dv7ywqBGQQNgUAiEevg/h8xc3TOHP08eX/5yAdD/Op0D/PnLBYMeF0ECAAAAsC0KGgAAAAC2ZWlB83/+z//RhAkTUm6LFi2ychMAAAAAkGTpd2i2bt2qOXPm6M4770y2eTweKzcBAAAAAEmWFjTbtm3Tcccdp4qKCivDAgAAAEBall5ytm3bNh111FFWhgQAAACAjCz7hMY0Tf3973/XW2+9pccee0yxWExnn322Fi1apKKiDHM+pBGLxRSLxaxKC1kwTTN56+4YtPeLS6YpScn+iXbTjKfEzLz8UL+u+qS/b3azPNv7hR6vgHNtXz35b1/vu6HO4yVxv7ux1nHdwxmm2T6mzeSOSKYpc5Cekzo9H5Kk9uffPKzdjJvJf+Om2WXf7NqtiNGXscmvY5sZNyVn+6mgEPMr9OdvQOR3+Lkgwzk07fnFNCUZg/acO5Ak/s729j19tutZVtDU1dWptbVVRUVFuv/++7Vr1y7dddddCoVC+qd/+qcex9m8ebNVKaEbTqdT/hFHqDUmuZwOGU63goGAgsFSbdq1TW1tbWnXc7vdcvkr1diQ+P354aqtrZWkQ+0lRYpFKxQMBpPx0i1vamqSGY8rGBzWZR+FXZ3uH2g+oFjUn3F5tvcLPV5B5xprv8w0EGjq831vC7UoFovpb5/vk9NhyHC61dTYKNM0ux1HxS5Hcl3TNOV1Sk37disWi8ntdmus36Ho/gbFm7+QJDmGSK5AUNt3bsr4eugvTqdTR1b45Iq2pLRHXSXaWR/o9R+NTHENp1vFLulAY6NiwS+S7S5jqEqjMR1obFL08HZJgUAg2d5l3x62WxGjL2OTX5q+FcMUK+T8Cv35s3t+gfbJNvfv3y9H2Eh7Dk13vpUk59ASlUZjCtXVJgujhFzPdciPmpqaftmOZQXN6NGj9d5778nv98swDB1//PGKx+O6+eabddttt8npdPYozqRJk7L6RAe5qWsK6c8b6lTqcelrx45Qmc+nsrIyVY3p+ntQdU0hlQ8dmvyflYkTJ8rpdCbbfcUuOV0ulZWVpcQ7fLnf3z4RV3d90t0vHVLa5fJs7xd6vILO1dl+KvH5/H2+70P9PkXiht7YGkiOW395eY/HUWJdSZo/tUrV1YfGuhHYJQ0bKnkPNniHSr4yTR4zJvcXmwWMwC6p5o+HZuf2+KTqBSqvrrY2riT5RssYN1vl5eWSp8Mbi3K/DJdT/nK/5Iknm02fT3FJPp9PRqI9Q9+s2q2I0ZexyS+lzYybCkhyFmh+hf78DYT8fD6f9gfrNWzYMBlDMp9DO51vJWlopQwzLNffX009H1l0rkP/icViqqmpUXV1dY9rgI4ikUhWH3JY+qMA5QffVCSMHz9e4XBYTU1NGjZsWI9iOJ3OXu04escwDAXDMRmGkXxsGEa3x6C9n0Om2k9mieOWaDcMR9p4nZcf6tdVn/T3jW6WZ3u/0OMVcK7tqyf/7Y99Tx232Y2jYDjWqf/BhkO3jo8L5ZxkGFIkKIUaDz22Ir/D40pScbmk9tdv8vlo75y2Pe44eCwchhyG0WXf7NqtiNGXscmvY1vc0WFJAeZX6M/fgMjv8HNBpnPU4efbjjEOPx8V2rkYPdbb9/XZrmPZjwL8v//3/3TiiSeqtbU12bZlyxaVl5f3uJgBAAAAgGxYVtBMmzZNHo9H//RP/6RPP/1Uf/nLX3Tvvffqe9/7nlWbAAAAAIAUll1yVlpaqieeeEL33HOPvvnNb2rIkCH67//9v1PQAAAAAOgzln6H5thjj9WTTz5pZUgAAAAAyMjSiTUBAAAAoD9R0AAAAACwLQoaAINWyq+FAgCsZ1j0VtOqOBiQLP0ODfKjqTWiYCgqSSordsnvtW5i0kRsw5A8LodCbXE5DUPRmNn9ykABK3Y75DAM7WpoSXndhKNxtYXbX09ud1yefCTX2pg6qZwkGU4pFs0thtQ+QZ23PIfkAKCHXF5JhtS4I7U92/NZpjjZns/SnRc5Jw4IFDQDQDAU1boNdZLaZz63sqBJxC71uDTrmOF6bdOe5H3AzoqcDjWHo/qPbV9o7sTK5OsmGotr+xctkqSjvEPzU9CEA1LN6tQ/vL7R0tGzc4txcLZt/ngD6BfOIinSLNW+ktv5LF2c3pzPDj8vck4cMChoBohAKIv/6cghdl9uB8iHlkisU1skFs9DJocJB6TWhkOPPf7cYwBAPlhxPksXJ18xUHC4IBEAAACAbVHQAAAAALAtChoAAAAAtkVBAwAAAMC2KGgAAAAA2BYFDQAAAADboqAZYKyY+TzbGA5H7htlxnZYoU/G0UCbnXqg7Q8AYNBjHpoBpOPM55JSZj/vqKk1omAoKqdhKBozU5Z5XIdipFuebpslXq/qmkJyORzd9u8u755sE0inT8aRO9Ms1472id6ioc7rZJp1uosZqhOvSUnyeV3y5Z55eulm2852xm4AAAoMBc0Akpj5/LVNeyRJ86dWpS1ogqGo1m2oU6nHpVnHDE9Z5u4QI93ydNtsjcT0p9o9Kit2d9u/u7x7sk0gnb4YR0Z3s1wf3t7VrNNdzFCdeE1K0oXTRvddQZNuf7KdsRsAgAJDQTMABULd/29rd316EiO1f5uMHK/3yXabQDp9Mo4yzXKd7YzTXfTv1/HfMY/eztgNAECB4GJqAAAAALZFQQMAAADAtihoAAAAANgWBQ0AAAAA26KgAQAAAGBbFDQAAAAAbIuCZgDrk1nTgQLlcTnkchjyFbtU6nHJ5TBU7OYUBwADlsE5Hu2YhyaDjjN3lxW70k5QmQ89zavjrOnd9QXsrtjt0MlHulURr9fZY9rkdEQ1Il6v048u1nt1zG8EAAOOyyvJkBp3pLYbjvZJhKOhw9qdUiyLvwetjakTJyd4fOknT0ZeUdBk0HHm7vlTqwqmGOhpXh1nTe+uL2B3RU6HimPNMj9cp5a6f6jI6ZA5cqRKxp4vt9Ob7/QAAFZzFkmRZqn2ldTCwzdaOnp25vaeCgekmtWpMTw+qXoBBU0BoqDpQqHOXJ9NXoW6D0BfiIea1HZgvwynQ3F/cb7TAQD0tXBAam049Njj77o9l9goWFx8CAAAAMC2KGgAAAAA2BYFDQAAAADboqABAAAAYFsUNAAAAABsi4IGAAAAgG3xs83oc4aR7wysMxhmo0+3j75ilzyunu9nkTP3GIWgV0M3m5mrD/Z1Gu3PjyQ5DEMy+2h7AIBUnEMHBAqaHDS1RhQ8OM9LWbEr7cSViT7dLT88RqLdMNrfYIba4nIahqKxbN7p5J/H5ZDDMLSrocWW+Xc0GGajz7SPZ49pU5u7VI4evMN3GNKMIxw5xSgELochGYZipqlwOCZnNJ4syMKxuFymqUgkJrXFFIubMiQ5vUVymZL5xXa5nR12NM0M1WHDI6cpxeo/k980dfaYNkmSP7pXUg/HU8aZsrOcERsABqO+Poe2NqZOzJng8aWfnDNd/2z6dtXfivwKGAVNDoKhqNZtqJMkzZ9albZgCYaieqN2r+ZOrMy4PF2MRHupx6VZxwzXa5v2JO/bidvpUHM4atv8OxoMs9Gn28eYv1htoSIVT7tYpjG02xgOw5A7dkDmhy/1OkYhcBiGovG4FI1rd6BVle5DBU0sFlc8GtfeYFjlRly7GlrldhoaVe6SWoMyt7wid7z5ULA0M1TH5FK8Jaj963+vMqNFLftbJEnm+AnScXN7lmR3M2UDADLr63NoOCDVrE6N7fFJ1QvSFwyH98+mb3f9rcivgFHQ5CgQ6r6Cb4nEehWjY3tPtlPI7J5/R4NhNvqO+2i6vWoLeZTtnloRoxCYphSNd/5k0TSlWDwuOaVILK7EVxJNUzJbA1K84x+I9DNUm6YUPtCgUmeL2g60F0BmKJh9klbMiA0Ag1VfnkMPj21l/2xj91WMAsCFgwAAAABsi4IGAAAAgG1ZWtCEw2EtWbJEM2bM0Mknn6xVq1ZZGR4AAAAAUlj6HZp7771XH330kX7zm9+orq5OP/rRj1RVVaWzzz7bys0AAAAAgCQLC5qWlha98MIL+vWvf63Jkydr8uTJ+uSTT/TMM89Q0AAAAADoE5YVNLW1tYpGo5o2bVqybfr06Xr00UcVj8flcGS+us00D/2CUFtbm1Up5SQWbZPHEU/ej0Qive7jNuJZx0i0FxlxxdPc79g/m77dxXYpljZG5vtSPBaVx2FaFK/n962O3V28IiOuWDyuNodXZlGpTIehNodX8Xg87bp23Pd0+xh1Fst0eRSLx2V2EcPrUvv60TY5Yr2L0dt9z/bYdLyf8vqMxtTmKD4Uw/AoHosr7vKqzelQxNF+ymwzPNLB9qhDMouMZH8zFpfpKFak41wyRpEUjUtGseQoyRgj0R45rG+mGGnbsm23IoakuFGkWMyU0yiWowDzK/Tnb6DkFzdNRY1iRaJxGQWYX6E/fwMhv7hRrKi7VBFHiRwFmF9uMYqlaExK834urWgsuxiH98+mb3/kl4VYrP0XfiORiJxOZ9brd6wHOtYJmRhmT3r1wB//+Ef97Gc/09tvv51s27Ztm+bNm6d33nlHw4YNy7huc3OzamtrrUgDAAAAwAAxceJEDRkypMs+lv0oQGtrq4qKUieOTDxO98kEAAAAAOTKskvOPB5Pp8Il8bi4uOvp9LxeryZOnNiekMslwzCsSgsAAACAjZimqWi0/dJtr9fbbX/LCpqRI0eqoaFB0WhULld72Pr6ehUXF8vn83W5rsPh6PajJAAAAACDg8fj6XFfyy45O/744+VyubRhw4Zk2/r161VdXd3lDwIAAAAAQG9ZVml4vV5dcMEFWrp0qT788EP96U9/0qpVq3T55ZdbtQkAAAAASGHZr5xJ7T8MsHTpUr322msqLS3VVVddpYULF1oVHgAAAABSWFrQAAAAAEB/4sstAAAAAGyLggYAAACAbVHQAAAAALAtChp0yTRN/fKXv9RJJ52kmTNn6t5771U8Hs/Yf+fOnVq4cKGmTp2qefPm6a233kpZ/uSTT+q0007TlClTdNVVV+mzzz7r4z2AFaweB2vWrNHZZ5+tadOm6Vvf+pbWr1/f17uAHFk9BhLWrVunyy67rK/ShgXC4bCWLFmiGTNm6OSTT9aqVasy9t28ebO+9a1vacqUKfrmN7+pjz76KGX5K6+8otNPP11TpkzRddddp/379/d1+rCAlWMgYcWKFbr11lv7KmX0AavGgWmaWrlypebOnasTTjhB3/3ud7V169bckjOBLjzxxBPm7NmzzQ8++MB85513zJNPPtl8/PHH0/aNx+Pm+eefby5evNjcunWr+eijj5pTpkwxP//8c9M0TfOll14yp0+fbv7f//t/zb///e/mjTfeaJ511llmPB7vz11CL1g5Dv7yl7+YX/nKV8yXXnrJ/Oyzz8xf/epX5gknnGDu3r27P3cJWbJyDCS888475pQpU8xLL720P3YBvfSzn/3MPP/8882PPvrIfO2118xp06aZf/jDHzr1a25uNr/2ta+ZP//5z82tW7ead955pzlr1iyzubnZNE3T3Lhxo/mVr3zFfPHFF80tW7aYl156qXn11Vf39+6gF6waAwkvv/yyefzxx5s/+tGP+msXYAGrxsGzzz5rnnjiieYbb7xhfvrpp+aSJUvM0047zWxpael1bhQ06NLs2bPNNWvWJB+vXbvWnDNnTtq+//Ef/2FOnTo15cT13e9+13zwwQdN0zTNp59+2nz++eeTy7Zs2WIed9xx5r59+/ooe1jFynFwww03mHfccUfKOmeeeab5u9/9rg8yh1WsHAOmaZr/8i//Yn75y182zzvvPAqaAtbc3GxWV1eb7777brLt4YcfTnvMXnjhBXPu3LnJ/6SKx+PmGWeckRw3N998c8ob2Lq6OnPChAnmjh07+ngvkAsrx0BbW5t5xx13mNXV1eaZZ55JQWMjVo6Db33rW+Zjjz2W7B+JRMypU6eab731Vq/z45IzZLRnzx794x//0H/7b/8t2TZ9+nR9/vnn2rt3b6f+Gzdu1KRJk1RSUpLSf8OGDZKk73znO7r44oslScFgUM8++6yOPfZYDRs2rG93BDmxehx873vf0xVXXNFpvWAwaH3ysITVY0CS3n77bT3xxBM688wz+zR35Ka2tlbRaFTTpk1Ltk2fPl0bN27sdMnhxo0bNX36dBmGIUkyDEMnnHBC8rhv3LhRM2bMSPYfNWqUqqqqtHHjxr7fEfSalWOgpaVFH3/8sf7t3/4tJR4Kn5Xj4JZbbtH8+fOT/Q3DkGmaOb0PoKBBRvX19ZKkysrKZNuIESMkSbt3707bv2NfSRo+fHinvqtXr9aMGTP04osv6o477kgOeBQmq8fB5MmTddRRRyWXvfnmm/rss8900kknWZ06LNIX54LnnntOM2fO7It0YaH6+noNHTpURUVFybYRI0YoHA6rsbGxU9+ujvvevXt79DcChcXKMeDz+fT8889r4sSJfZ43rGXlOJgxY4aOOOKI5LIXXnhB0WhU06dP73V+rl6viQEhFAppz549aZe1tLRIUsrgTdyPRCKd+re2tqb0TfQ/vO+sWbP04osvas2aNfqf//N/6sUXX9SRRx6Z034gN/kYB5K0Y8cO3XbbbTr//PM1efLkXueP3OVrDKCwZTqWUudj391xD4VCjAsbsnIMwL76ahxs3LhRv/jFL3TVVVepoqKi1/lR0AxyGzdu1OWXX5522c033yypfaB6PJ7kfUnyer2d+ns8nk5VeiQSUXFxcUpbVVWVqqqqdPzxx+v999/X2rVr9cMf/jDXXUEO8jEO/v73v+uKK67QkUceqbvuuivXXUCO8jEGUPg8Hk+nNyGJx4cfz0x9E/0yLU83hlA4rBwDsK++GAf/9V//pe9///s69dRT9b/+1//KKT8KmkHuxBNP1Mcff5x22Z49e7R8+XLV19drzJgxkg5depKuih45cmSnn93bt29f8mPHd999V5WVlRo3bpyk9msmx40bp4aGBsv2B73Tn+NAkj755BMtXLhQRx55pB5//HH+2BWA/h4DsIeRI0eqoaFB0WhULlf7W4b6+noVFxfL5/N16rtv376Uto7HPdPyXP5XFn3PyjEA+7J6HLz33nu65ppr9LWvfU333XefHI7cvgXDd2iQ0ciRI1VVVZUyR8j69etVVVWV9uQ0ZcoUbdq0SaFQKKX/lClTJEm//vWv9dRTTyWXxWIx1dbWavz48X23E8iZ1eNg7969uvLKKzV27Fg98cQTKi0t7fudQE6sHgOwj+OPP14ulyvlBx3Wr1+v6urqTm9ApkyZov/6r/+SaZqS2uea+Otf/5o87lOmTEkZQ//4xz/0j3/8g3FR4KwcA7AvK8fB3/72N1177bU65ZRTdP/998vtduecHwUNuvQ//sf/0C9/+Uu99957eu+993TfffelXJayf/9+NTc3S5JmzpypUaNG6bbbbtMnn3yilStX6sMPP9SCBQskSZdccol+//vf6+WXX9ann36qpUuXKhQK6YILLsjHriELVo6DX/ziF4rH47r77rvV0tKi+vp61dfXJ9dHYbJyDMA+vF6vLrjgAi1dulQffvih/vSnP2nVqlXJY19fX58sXM8++2wFAgHdfffd2rp1q+6++261trbqnHPOkdQ+hl566SW98MILqq2t1S233KLTTjuN71AWOCvHAOzLynFwxx13JP9GNDQ0JN8HdPxPsKz1+gefMShEo1HznnvuMWfMmGGeeOKJ5vLly1MmwpwzZ07K3BKfffaZ+Z3vfMf88pe/bJ577rnm22+/nRLvhRdeMM8880yzurravOyyy8ytW7f2276g96waB/F43PzKV75iHnfccZ1uHddH4bH6XJDw4IMPMg9NgWtpaTFvueUWc+rUqebJJ59sPvnkk8llxx13XMr8RBs3bjQvuOACs7q62lywYIG5adOmlFhr1qwxZ8+ebU6dOtW87rrrzP379/fXbiAHVo6BhB/96EfMQ2MzVoyDvXv3pn0PcPj62TJM8+DnQQAAAABgM1xyBgAAAMC2KGgAAAAA2BYFDQAAAADboqABAAAAYFsUNAAAAABsi4IGAAAAgG1R0AAAAACwLQoaAAAAALZFQQMA6HNz587VhAkTkrfJkyfr7LPP1lNPPZXs09LSoptuuknTp0/X/PnztWnTppQYv//97zV37tx+zhwAUOhc+U4AADA4LFmyRPPmzZMkRaNRvfvuu7r99ttVXl6uCy64QI8++qg+/fRT/du//ZuefPJJLVmyRC+99FKeswYAFDo+oQEA9IuysjJVVFSooqJCo0aN0oUXXqivfvWreu211yRJn3zyiU444QSNHz9es2fP1o4dO/KcMQDADihoAAB543K55Ha7JUknnnii1q5dq9raWv3rv/6rzjnnnDxnBwCwAwoaAEC/a2tr02uvvaa3335bX//61yVJl1xyiXw+ny644AJVVFToJz/5SZ6zBADYAd+hAQD0i5/85Ce68847JUmhUEjFxcX67ne/q/nz5ysUCunGG29ULBZTSUmJKioq5PF41NzcrCFDhuQ5cwBAIaOgAQD0i0WLFunMM8+UJHk8HlVUVMjpdEqS7rnnHu3cuVNr167V22+/rZtuukmzZs3SL3/5S5111lm6/vrr85k6AKCAUdAAAPrF8OHDNXbs2LTL/vCHP+jOO+/U0KFDdd555+n999/X4sWLFQwGdffdd/dzpgAAO+E7NACAvCsuLtb+/fuTj5csWSLDMDRixAhNnDgxj5kBAAodn9AAAPJuwYIFevjhh/WlL31JRxxxhFasWKEhQ4YoEono5ptv1vLlyyW1f/fmzTffTFnX7/drypQp+UgbAFAAKGgAAHl33XXXKRQKafHixQqHw5o1a5aeffZZ7du3T0uXLlUwGJQkffHFF/r+97+fsu4JJ5yg5557Lh9pAwAKgGGappnvJAAAyMQ0TRmGke80AAAFiu/QAAAKGsUMAKArFDQAAAAAbIuCBgAAAIBtUdAAAAAAsC0KGgAAAAC2RUEDAAAAwLYoaAAAAADYFgUNAAAAANuioAEAAABgWxQ0AAAAAGyLggYAAACAbf1/01QPySOvvg8AAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntb pnl:\t\t-0.0237, -0.0236, 0.0037\n",
      "rl pnl:\t\t-0.0000, 0.0001, 0.0061\n",
      "ntb sharpe:\t\t-6.36965\n",
      "rl sharpe:\t\t0.01507\n",
      "ntb cvar:\t\t0.03160\n",
      "rl cvar:\t\t0.01384\n"
     ]
    }
   ],
   "source": [
    "plt_kwargs = {'bins': 100,\n",
    "              # 'range': (-0.04, 0.01),\n",
    "              'alpha': 0.6}\n",
    "\n",
    "plt.xlabel('P&L')\n",
    "# plt.hist(random_pnl, bins=100, range=(-0.25, 0.05), alpha=0.6, label='random')\n",
    "# plt.hist(random_pnl, bins=100, range=(-0.1, 0.05), alpha=0.6, label='random')\n",
    "# plt.hist(delta_pnl, **plt_kwargs, label='delta')\n",
    "plt.hist(ntb_pnl,**plt_kwargs, label='DDPG+NTB')\n",
    "plt.hist(rl_pnl, **plt_kwargs, label='DDPG')\n",
    "# plt.ylim(0, 150)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# r1, (m1, s1) = pnl_reward(random_pnl)\n",
    "r2, (m2, s2) = pnl_reward(ntb_pnl)\n",
    "r3, (m3, s3) = pnl_reward(rl_pnl)\n",
    "# print(f'random:\\t{r1:.4f}, {m1:.4f}, {s1:.4f}')\n",
    "print(f'ntb pnl:\\t\\t{r2:.4f}, {m2:.4f}, {s2:.4f}')\n",
    "print(f'rl pnl:\\t\\t{r3:.4f}, {m3:.4f}, {s3:.4f}')\n",
    "# print((r2-r1))\n",
    "# print((r3-r2))\n",
    "\n",
    "print(f'ntb sharpe:\\t\\t{sharpe_ratio(ntb_pnl):.5f}')\n",
    "print(f'rl sharpe:\\t\\t{sharpe_ratio(rl_pnl):.5f}')\n",
    "\n",
    "print(f'ntb cvar:\\t\\t{cvar(ntb_pnl):.5f}')\n",
    "print(f'rl cvar:\\t\\t{cvar(rl_pnl):.5f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(-0.022955209587592933, 0.00333743451844443)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntb_pnl.mean(), ntb_pnl.std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(-0.022750967764943068, 0.006194732383409264)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_pnl.mean(), rl_pnl.std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save('best_results/ntb_pnl_0614', ntb_pnl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "np.save('best_results/rl_pnl_eval_cash', rl_pnl,)\n",
    "np.save('best_results/ntb_pnl_eval_cash', ntb_pnl,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023253765816845084\n",
      "-0.02307192182905543\n"
     ]
    }
   ],
   "source": [
    "from Utils.prices import pnl_entropic_loss\n",
    "\n",
    "def loss(pnl, aversion=0.0):\n",
    "    return np.mean((1-np.exp(-aversion * pnl))/(aversion+1e-7))\n",
    "\n",
    "print(loss(ntb_pnl, aversion=1.1))\n",
    "print(loss(rl_pnl, aversion=1.1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02867360759284007 0.03344846486958373\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ntb_loss = pd.Series(-ntb_pnl)\n",
    "rl_loss = pd.Series(-rl_pnl)\n",
    "\n",
    "var95_ntb = ntb_loss.quantile(0.95)\n",
    "var95_rl = rl_loss.quantile(0.95)\n",
    "\n",
    "print(var95_ntb, var95_rl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030563063997726295\n"
     ]
    }
   ],
   "source": [
    "print(ntb_loss[ntb_loss >= var95_ntb].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03678241960186988\n"
     ]
    }
   ],
   "source": [
    "print(rl_loss[rl_loss >= var95_rl].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030563063997726295 0.03678241960186988\n"
     ]
    }
   ],
   "source": [
    "print(cvar(ntb_pnl, 0.95), cvar(rl_pnl, 0.95))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.randn(3,4).shape[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "rl_pnl = ntb_pnl.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## New Problem\n",
    "\n",
    "총 얼마 썼는가?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cash_eval(env, model=None):\n",
    "    obs = env.reset()\n",
    "    done, info = False, {}\n",
    "    cash = 0\n",
    "    while not done:\n",
    "        if model:\n",
    "            action, _ = model.predict(obs, deterministic=False)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        obs, reward, done, info = env.cashflow_pnl(action)\n",
    "        cash += info['raw_reward']\n",
    "        print(np.mean(info['raw_reward']))\n",
    "\n",
    "    return cash"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "ntb_cash = np.load('best_results/ntb_cash_0614.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "rl_cash = np.mean([cash_eval(eval_env, model) for _ in range(30)], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntb_pnl:\t-0.0152, -0.0151, 0.0030\n",
      "rl:\t\t-0.0230, -0.0229, 0.0057\n"
     ]
    }
   ],
   "source": [
    "plt_kwargs = {'bins': 100,\n",
    "              'range': (-0.04, -0.01),\n",
    "              'alpha': 0.6}\n",
    "\n",
    "plt.xlabel('P&L')\n",
    "plt.hist(ntb_cash,**plt_kwargs, label='ntb')\n",
    "plt.hist(rl_cash, **plt_kwargs, label='rl')\n",
    "# plt.ylim(0, 150)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# r1, (m1, s1) = pnl_reward(random_pnl)\n",
    "r2, (m2, s2) = pnl_reward(ntb_cash)\n",
    "r3, (m3, s3) = pnl_reward(rl_cash)\n",
    "# print(f'random:\\t{r1:.4f}, {m1:.4f}, {s1:.4f}')\n",
    "print(f'ntb_pnl:\\t{r2:.4f}, {m2:.4f}, {s2:.4f}')\n",
    "print(f'rl:\\t\\t{r3:.4f}, {m3:.4f}, {s3:.4f}')\n",
    "# print((r2-r1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "np.save('best_results/ntb_cash_0614', ntb_cash)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}